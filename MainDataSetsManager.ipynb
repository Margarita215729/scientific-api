{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mVPRxagiRVdA",
        "BIW9n9v6Tqh4",
        "4CiJTdHvT5jB"
      ],
      "authorship_tag": "ABX9TyNWFTkHABo/6Xcpm8/CNN31",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Margarita215729/scientific-api/blob/main/MainDataSetsManager.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Galaxy Data Aggregation Notebook\n",
        "_Сбор и объединение реальных данных галактик из обзоров JWST, SDSS DR17, DESI EDR и Euclid Q1 в единый CSV_\n",
        "\n",
        "**Цель:**  \n",
        "- Получить максимально полный набор галактик (RA, Dec, z) из реальных наблюдений  \n",
        "- Привести данные к единому формату и сохранить промежуточные CSV  \n",
        "- Объединить все данные в один итоговый файл  \n",
        "- Работать в среде Google Colab с ограничениями ~2.7 GB RAM, ~107 GB диск  \n",
        "- Данные охватывают z от 0 до ~3.5+  "
      ],
      "metadata": {
        "id": "6lb_mh8pQxQ4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy pandas astropy\n",
        "\n",
        "import sys\n",
        "import time\n",
        "import logging\n",
        "import warnings\n",
        "import csv\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from datetime import datetime\n",
        "import urllib.request\n",
        "import urllib.error\n",
        "import urllib.parse\n",
        "import io\n",
        "import re\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from astropy.io import fits, ascii\n",
        "from astropy.table import Table\n",
        "from astropy import units as u\n",
        "from astropy.cosmology import Planck18 as cosmo\n",
        "import sys\n",
        "import time\n",
        "import logging\n",
        "import warnings\n",
        "import csv\n",
        "import json\n",
        "import math\n",
        "import random\n",
        "from datetime import datetime\n",
        "import urllib.request\n",
        "import urllib.error\n",
        "import urllib.parse\n",
        "\n",
        "# Настройка рабочей среды\n",
        "OUTPUT_DIR = \"galaxy_data\"\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "# Настройка логирования\n",
        "logging.basicConfig(level=logging.INFO,\n",
        "                   format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
        "                   handlers=[\n",
        "                       logging.StreamHandler(),\n",
        "                       logging.FileHandler(os.path.join(OUTPUT_DIR, \"data_collection.log\"))\n",
        "                   ])\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "# Подавление предупреждений\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "id": "WwBiSTRNN_b1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d9ddc71-ca46-4cea-c486-f27417be8b04"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (2.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: astropy in /usr/local/lib/python3.11/dist-packages (7.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: pyerfa>=2.0.1.1 in /usr/local/lib/python3.11/dist-packages (from astropy) (2.0.1.5)\n",
            "Requirement already satisfied: astropy-iers-data>=0.2025.4.28.0.37.27 in /usr/local/lib/python3.11/dist-packages (from astropy) (0.2025.5.12.0.38.29)\n",
            "Requirement already satisfied: PyYAML>=6.0.0 in /usr/local/lib/python3.11/dist-packages (from astropy) (6.0.2)\n",
            "Requirement already satisfied: packaging>=22.0.0 in /usr/local/lib/python3.11/dist-packages (from astropy) (24.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Ограничения на количество строк для выборки (чтобы не загружать полностью многогигабайтные файлы)\n",
        "MAX_ROWS_EUCLID = None    # None = загрузить все\n",
        "MAX_ROWS_DESI   = None\n",
        "MAX_ROWS_SDSS   = None\n",
        "MAX_ROWS_JWST   = None\n",
        "MAX_ROWS_DES    = 1000000  # по умолчанию ограничим DES Y6 ~1e6 строк (для тестирования и производительности)\n",
        "\n",
        "CHUNK_SIZE = 1000000  # размер чанка для построчной загрузки больших таблиц (например, DES Y6)\n",
        "INCLUDE_SIM = False   # включать ли данные симуляции (файл 'galaxy_data/sim.csv'), можно изменить на True при необходимости\n",
        "\n",
        "# Константы для астрономических вычислений\n",
        "SPEED_OF_LIGHT = 299792.458  # км/с\n",
        "H0 = 67.74  # Постоянная Хаббла, км/с/Мпк\n",
        "OM0 = 0.3089  # Плотность материи\n",
        "\n",
        "def find_column_name(col_names, options):\n",
        "    \"\"\"\n",
        "    Найти имя колонки в списке col_names, соответствующее (без учета регистра) одному из вариантов в options.\n",
        "    Возвращает реальное имя колонки из col_names или None, если не найдено.\n",
        "    \"\"\"\n",
        "    col_names_upper = [name.upper() for name in col_names]\n",
        "    for opt in options:\n",
        "        opt_up = opt.upper()\n",
        "        if opt_up in col_names_upper:\n",
        "            idx = col_names_upper.index(opt_up)\n",
        "            return col_names[idx]\n",
        "    return None\n",
        "\n",
        "# Вспомогательные функции\n",
        "def download_file(url, filename, chunk_size=8192, headers=None):\n",
        "    \"\"\"\n",
        "    Скачивает файл с отображением прогресса\n",
        "\n",
        "    Parameters:\n",
        "    url - URL для скачивания\n",
        "    filename - имя файла для сохранения\n",
        "    chunk_size - размер блока данных\n",
        "    headers - дополнительные заголовки\n",
        "\n",
        "    Returns:\n",
        "    True, если скачивание успешно, False в противном случае\n",
        "    \"\"\"\n",
        "    if not headers:\n",
        "        headers = {\n",
        "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
        "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
        "            'Accept-Language': 'en-US,en;q=0.5',\n",
        "            'Connection': 'keep-alive',\n",
        "            'Upgrade-Insecure-Requests': '1',\n",
        "            'Cache-Control': 'max-age=0'\n",
        "        }\n",
        "\n",
        "    try:\n",
        "        logger.info(f\"Скачивание файла из {url}\")\n",
        "\n",
        "        # Создаем запрос с заголовками\n",
        "        req = urllib.request.Request(url, headers=headers)\n",
        "\n",
        "        # Открываем соединение\n",
        "        with urllib.request.urlopen(req) as response, open(filename, 'wb') as out_file:\n",
        "            # Получаем общий размер, если доступен\n",
        "            content_length = response.getheader('Content-Length')\n",
        "            total_size = int(content_length) if content_length else None\n",
        "\n",
        "            downloaded = 0\n",
        "            start_time = time.time()\n",
        "            last_update = start_time\n",
        "\n",
        "            while True:\n",
        "                chunk = response.read(chunk_size)\n",
        "                if not chunk:\n",
        "                    break\n",
        "\n",
        "                out_file.write(chunk)\n",
        "                downloaded += len(chunk)\n",
        "\n",
        "                # Обновляем индикатор прогресса\n",
        "                current_time = time.time()\n",
        "                if current_time - last_update > 2 and total_size:\n",
        "                    percent = downloaded * 100 / total_size\n",
        "                    elapsed = current_time - start_time\n",
        "                    speed = downloaded / elapsed / 1024 if elapsed > 0 else 0\n",
        "                    logger.info(f\"Загружено {downloaded/(1024*1024):.1f} MB / {total_size/(1024*1024):.1f} MB ({percent:.1f}%) скорость: {speed:.1f} KB/s\")\n",
        "                    last_update = current_time\n",
        "\n",
        "        logger.info(f\"Файл успешно загружен и сохранен как {filename}\")\n",
        "        return True\n",
        "\n",
        "    except urllib.error.HTTPError as e:\n",
        "        logger.error(f\"HTTP ошибка при скачивании {url}: {e.code} {e.reason}\")\n",
        "        return False\n",
        "    except urllib.error.URLError as e:\n",
        "        logger.error(f\"URL ошибка при скачивании {url}: {e.reason}\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Ошибка при скачивании {url}: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def rad_to_deg(rad):\n",
        "    \"\"\"Перевод радиан в градусы\"\"\"\n",
        "    return rad * 180.0 / math.pi\n",
        "\n",
        "def deg_to_rad(deg):\n",
        "    \"\"\"Перевод градусов в радианы\"\"\"\n",
        "    return deg * math.pi / 180.0\n",
        "\n",
        "def e_z(z, Om0):\n",
        "    \"\"\"Функция E(z) для плоской ΛCDM модели\"\"\"\n",
        "    return math.sqrt(Om0 * (1 + z)**3 + (1 - Om0))\n",
        "\n",
        "def comoving_distance(z, H0=H0, Om0=OM0):\n",
        "    \"\"\"\n",
        "    Вычисляет комовское расстояние для заданного красного смещения\n",
        "\n",
        "    Parameters:\n",
        "    z - красное смещение\n",
        "    H0 - постоянная Хаббла, км/с/Мпк\n",
        "    Om0 - плотность материи\n",
        "\n",
        "    Returns:\n",
        "    Комовское расстояние в Мпк\n",
        "    \"\"\"\n",
        "    # Проверка входных данных\n",
        "    try:\n",
        "        z = float(z)\n",
        "        if z < 0:\n",
        "            logger.warning(f\"Отрицательное красное смещение: {z}, используем |z|\")\n",
        "            z = abs(z)\n",
        "        elif z > 20:\n",
        "            logger.warning(f\"Слишком большое красное смещение: {z}, ограничиваем до 20\")\n",
        "            z = 20.0\n",
        "    except (ValueError, TypeError):\n",
        "        logger.warning(f\"Некорректное красное смещение: {z}, используем z=1.0\")\n",
        "        z = 1.0\n",
        "\n",
        "    # Для малых z используем приближение\n",
        "    if z < 0.01:\n",
        "        return SPEED_OF_LIGHT * z / H0\n",
        "\n",
        "    # Для больших z используем численное интегрирование\n",
        "    n_steps = 100\n",
        "    dz = z / n_steps\n",
        "    integral = 0\n",
        "\n",
        "    for i in range(n_steps):\n",
        "        z_i = i * dz\n",
        "        integral += 1 / e_z(z_i, Om0) * dz\n",
        "\n",
        "    # Умножаем на c/H0\n",
        "    return SPEED_OF_LIGHT / H0 * integral\n",
        "\n",
        "def sky_to_cartesian(ra, dec, z):\n",
        "    \"\"\"\n",
        "    Преобразует астрономические координаты в декартовы\n",
        "\n",
        "    Parameters:\n",
        "    ra, dec - прямое восхождение и склонение в градусах\n",
        "    z - красное смещение\n",
        "\n",
        "    Returns:\n",
        "    x, y, z - декартовы координаты в Мпк или None в случае ошибки\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Проверка входных данных\n",
        "        ra = float(ra)\n",
        "        dec = float(dec)\n",
        "        z = float(z)\n",
        "\n",
        "        # Нормализация значений\n",
        "        if ra < 0 or ra > 360:\n",
        "            ra = ra % 360\n",
        "        if dec < -90 or dec > 90:\n",
        "            dec = max(-90, min(90, dec))\n",
        "        if z <= 0:\n",
        "            logger.warning(f\"Некорректное красное смещение: {z}, используем z=0.1\")\n",
        "            z = 0.1\n",
        "\n",
        "        # Перевод в радианы\n",
        "        ra_rad = deg_to_rad(ra)\n",
        "        dec_rad = deg_to_rad(dec)\n",
        "\n",
        "        # Комовское расстояние\n",
        "        dist = comoving_distance(z)\n",
        "\n",
        "        # Перевод в декартовы координаты\n",
        "        x = dist * math.cos(dec_rad) * math.cos(ra_rad)\n",
        "        y = dist * math.cos(dec_rad) * math.sin(ra_rad)\n",
        "        z_cart = dist * math.sin(dec_rad)\n",
        "\n",
        "        return x, y, z_cart\n",
        "\n",
        "    except (ValueError, TypeError) as e:\n",
        "        logger.warning(f\"Ошибка при преобразовании координат: {str(e)} (ra={ra}, dec={dec}, z={z})\")\n",
        "        return None\n",
        "\n",
        "def parse_csv_data(csv_content, delimiter=',', skip_lines=0):\n",
        "    \"\"\"\n",
        "    Парсит CSV данные и возвращает список словарей\n",
        "\n",
        "    Parameters:\n",
        "    csv_content - содержимое CSV файла (строка)\n",
        "    delimiter - разделитель полей\n",
        "    skip_lines - количество строк для пропуска в начале\n",
        "\n",
        "    Returns:\n",
        "    Список словарей с данными\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Пропускаем начальные строки, если нужно\n",
        "        lines = csv_content.splitlines()\n",
        "        if skip_lines > 0:\n",
        "            if len(lines) <= skip_lines:\n",
        "                logger.warning(f\"CSV содержит только {len(lines)} строк, но нужно пропустить {skip_lines}\")\n",
        "                return []\n",
        "            lines = lines[skip_lines:]\n",
        "\n",
        "        # Удаляем пустые строки\n",
        "        lines = [line.strip() for line in lines if line.strip()]\n",
        "\n",
        "        if not lines:\n",
        "            logger.warning(\"CSV не содержит данных\")\n",
        "            return []\n",
        "\n",
        "        # Читаем CSV\n",
        "        csv_reader = csv.DictReader(io.StringIO('\\n'.join(lines)), delimiter=delimiter)\n",
        "        data = [row for row in csv_reader]\n",
        "\n",
        "        logger.info(f\"Прочитано {len(data)} строк из CSV\")\n",
        "        return data\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Ошибка при парсинге CSV: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "def save_data_to_csv(data, filename, fieldnames=None):\n",
        "    \"\"\"\n",
        "    Сохраняет список словарей в CSV файл\n",
        "\n",
        "    Parameters:\n",
        "    data - список словарей\n",
        "    filename - путь к файлу\n",
        "    fieldnames - список имен полей (столбцов)\n",
        "\n",
        "    Returns:\n",
        "    True если успешно, False в случае ошибки\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if not data:\n",
        "            logger.warning(\"Нет данных для сохранения\")\n",
        "            return False\n",
        "\n",
        "        # Определяем имена полей\n",
        "        if not fieldnames:\n",
        "            # Находим все возможные ключи в данных\n",
        "            all_keys = set()\n",
        "            for row in data:\n",
        "                all_keys.update(row.keys())\n",
        "\n",
        "            # Исключаем пустые ключи\n",
        "            all_keys = [key for key in all_keys if key]\n",
        "\n",
        "            # Приоритетные поля в начале списка\n",
        "            priority_fields = ['ra', 'dec', 'z', 'source', 'x', 'y', 'z_cart']\n",
        "            fieldnames = [field for field in priority_fields if field in all_keys]\n",
        "            fieldnames.extend([field for field in all_keys if field not in priority_fields])\n",
        "\n",
        "        # Проверяем данные перед сохранением\n",
        "        valid_data = []\n",
        "        for row in data:\n",
        "            # Создаем новую запись только с полями из fieldnames\n",
        "            valid_row = {}\n",
        "            for field in fieldnames:\n",
        "                if field in row:\n",
        "                    valid_row[field] = row[field]\n",
        "\n",
        "            valid_data.append(valid_row)\n",
        "\n",
        "        # Сохраняем данные\n",
        "        with open(filename, 'w', newline='') as csvfile:\n",
        "            writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
        "            writer.writeheader()\n",
        "            writer.writerows(valid_data)\n",
        "\n",
        "        logger.info(f\"Сохранено {len(valid_data)} строк в {filename}\")\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Ошибка при сохранении CSV: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "def load_data_from_csv(filename):\n",
        "    \"\"\"\n",
        "    Загружает данные из CSV файла в список словарей\n",
        "\n",
        "    Parameters:\n",
        "    filename - путь к файлу\n",
        "\n",
        "    Returns:\n",
        "    Список словарей с данными\n",
        "    \"\"\"\n",
        "    try:\n",
        "        data = []\n",
        "        with open(filename, 'r', newline='') as csvfile:\n",
        "            reader = csv.DictReader(csvfile)\n",
        "            for row in reader:\n",
        "                # Преобразуем строковые значения в числа, где нужно\n",
        "                numeric_fields = ['ra', 'dec', 'z', 'x', 'y', 'z_cart']\n",
        "                for field in numeric_fields:\n",
        "                    if field in row and row[field]:\n",
        "                        try:\n",
        "                            row[field] = float(row[field])\n",
        "                        except ValueError:\n",
        "                            pass\n",
        "                data.append(row)\n",
        "\n",
        "        logger.info(f\"Загружено {len(data)} строк из {filename}\")\n",
        "        return data\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        logger.error(f\"Файл не найден: {filename}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        logger.error(f\"Ошибка при загрузке CSV: {str(e)}\")\n",
        "        return []\n",
        "\n",
        "\n",
        "def get_euclid_data():\n",
        "    EUCLID_URL = \"https://irsa.ipac.caltech.edu/ibe/data/euclid/q1/catalogs/MER_FINAL_CATALOG/102018211/EUC_MER_FINAL-CAT_TILE102018211-CC66F6_20241018T214045.289017Z_00.00.fits\"\n",
        "    \"\"\"Загрузить Euclid Q1 MER Final (FITS) и сконвертировать в CSV.\"\"\"\n",
        "    output_path = os.path.join(OUTPUT_DIR, \"euclid.csv\")\n",
        "    if os.path.exists(output_path):\n",
        "        print(\"Euclid: CSV уже есть — пропускаем.\")\n",
        "\n",
        "    print(\"Euclid: поиск HDU и чтение таблицы...\")\n",
        "    # пытаемся найти в FITS бинарную таблицу с данными\n",
        "    tbl = None\n",
        "    for ext in [1, 2, 3]:\n",
        "        try:\n",
        "            candidate = Table.read(EUCLID_URL, hdu=ext)\n",
        "            if len(candidate) > 10:\n",
        "                tbl = candidate\n",
        "                print(f\"  → HDU={ext}, строк={len(tbl)}\")\n",
        "                break\n",
        "        except Exception:\n",
        "            continue\n",
        "    if tbl is None:\n",
        "        raise RuntimeError(\"Euclid: не удалось найти HDU с данными\")\n",
        "\n",
        "\n",
        "    # Определяем колонки RA, DEC, REDSHIFT\n",
        "    cols = tbl.colnames\n",
        "    ra_col  = find_column_name(cols, [\"RA\",\"RAJ2000\",\"ALPHA_J2000\",\"ra_deg\",\"ra\",\"Ra\",\"ra(deg)\"])\n",
        "    dec_col = find_column_name(cols, [\"DEC\",\"DECJ2000\",\"DEJ2000\",\"delta_j2000\",\"dec_deg\",\"dec\",\"Dec\",\"dec(deg)\"])\n",
        "    z_col   = find_column_name(cols, [\"Z\",\"REDSHIFT\",\"Z_SPEC\",\"z_spec\",\"PHOTOZ\",\"redshift\",\"z\",\"z_phot\",\"Redshift\"])\n",
        "    if not ra_col or not dec_col:\n",
        "        raise RuntimeError(f\"Euclid: не найдены RA/DEC в {cols}\")\n",
        "\n",
        "    # Красное смещение в MER Final обычно отсутствует — заполняем NaN\n",
        "    z_col = find_column_name(cols, [\"REDSHIFT\", \"Z\", \"Z_SPEC\", \"Z_SPEC_PHOT\", \"PHOTOZ\", \"z_phot\"])\n",
        "    ra_data  = np.array(tbl[ra_col])\n",
        "    dec_data = np.array(tbl[dec_col])\n",
        "    z_data   = np.array(tbl[z_col]) if z_col else np.full(len(tbl), np.nan)\n",
        "\n",
        "    # Собираем DataFrame и обрезаем, если надо\n",
        "    df = pd.DataFrame({\"RA\": ra_data, \"DEC\": dec_data, \"redshift\": z_data})\n",
        "    if MAX_ROWS_EUCLID is not None and len(df) > MAX_ROWS_EUCLID:\n",
        "        df = df.iloc[:MAX_ROWS_EUCLID]\n",
        "\n",
        "    # Сохраняем\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"Euclid: сохранено {len(df)} объектов → {output_path}\")\n",
        "    return True\n",
        "\n",
        "\n",
        "def get_sdss_data():\n",
        "    \"\"\"Загрузить SDSS DR17 spectro (FITS) и сконвертировать в CSV.\"\"\"\n",
        "    SDSS_URL = \"https://data.sdss.org/sas/dr17/sdss/spectro/redux/specObj-dr17.fits\"\n",
        "    output_path = os.path.join(OUTPUT_DIR, \"sdss.csv\")\n",
        "    if os.path.exists(output_path):\n",
        "        print(\"SDSS: CSV уже есть — пропускаем.\")\n",
        "        return output_path\n",
        "\n",
        "    print(\"SDSS: поиск HDU и чтение таблицы...\")\n",
        "    tbl = None\n",
        "    for ext in [1,2,3]:\n",
        "        try:\n",
        "            candidate = Table.read(SDSS_URL, hdu=ext)\n",
        "            if len(candidate) > 10:\n",
        "                tbl = candidate\n",
        "                print(f\"SDSS: выбрана HDU={ext}, строк={len(tbl)}\")\n",
        "                break\n",
        "        except Exception:\n",
        "            continue\n",
        "    if tbl is None:\n",
        "        raise RuntimeError(\"SDSS: не удалось найти подходящий HDU с данными\")\n",
        "\n",
        "    cols = tbl.colnames\n",
        "    ra_col  = find_column_name(cols, [\"RA\",\"RAJ2000\",\"ALPHA_J2000\",\"ra_deg\"])\n",
        "    dec_col = find_column_name(cols, [\"DEC\",\"DECJ2000\",\"DEJ2000\",\"delta_j2000\",\"dec_deg\"])\n",
        "    z_col   = find_column_name(cols, [\"Z\",\"REDSHIFT\",\"Z_NOQSO\",\"Z_SPEC\",\"PHOTOZ\",\"redshift\"])\n",
        "\n",
        "    data = {\n",
        "        \"RA\": np.array(tbl[ra_col]),\n",
        "        \"DEC\": np.array(tbl[dec_col]),\n",
        "        \"redshift\": np.array(tbl[z_col]) if z_col else np.full(len(tbl), np.nan)\n",
        "    }\n",
        "    df = pd.DataFrame(data)\n",
        "    if MAX_ROWS_SDSS and len(df) > MAX_ROWS_SDSS:\n",
        "        df = df.iloc[:MAX_ROWS_SDSS]\n",
        "\n",
        "    df.to_csv(output_path, index=False)\n",
        "    print(f\"SDSS: сохранено {len(df)} объектов → {output_path}\")\n",
        "    return True\n",
        "\n",
        "\n",
        "\n",
        "def get_desi_data():\n",
        "  DESI_URL = \"https://data.desi.lbl.gov/public/dr1/survey/catalogs/dr1/LSS/iron/LSScats/v1.2/ELG_LOPnotqso_NGC_clustering.dat.fits\"\n",
        "  output_path = os.path.join(OUTPUT_DIR, \"desi.csv\")\n",
        "  if os.path.exists(output_path):\n",
        "      print(\"Файл DESI (desi.csv) уже существует, пропуск загрузки.\")\n",
        "      return output_path\n",
        "  print(\"Скачивание данных DESI DR1 ELG clustering...\")\n",
        "  hdul = fits.open(DESI_URL, memmap=True)\n",
        "  data = hdul[1].data\n",
        "  # Имена колонок\n",
        "  ra_col  = find_column_name(hdul[1].columns.names, [\"RA\", \"RAJ2000\", \"ALPHA_J2000\"])\n",
        "  dec_col = find_column_name(hdul[1].columns.names, [\"DEC\", \"DECJ2000\", \"DEJ2000\", \"DELTA_J2000\"])\n",
        "  z_col   = find_column_name(hdul[1].columns.names, [\"Z\", \"REDSHIFT\", \"Z_SPEC\", \"ZSPEC\", \"Z_PHOT\", \"ZPHOT\", \"PHOTOZ\", \"Z_MEAN\"])\n",
        "  ra_data = data[ra_col]\n",
        "  dec_data = data[dec_col]\n",
        "  if z_col:\n",
        "      z_data = data[z_col]\n",
        "  else:\n",
        "      z_data = np.full(len(ra_data), np.nan)\n",
        "    # Обработка масок\n",
        "  if hasattr(ra_data, 'mask'):\n",
        "      ra_data = ra_data.filled(np.nan)\n",
        "  if hasattr(dec_data, 'mask'):\n",
        "      dec_data = dec_data.filled(np.nan)\n",
        "  if hasattr(z_data, 'mask'):\n",
        "      z_data = z_data.filled(np.nan)\n",
        "  df = pd.DataFrame({\"RA\": ra_data, \"DEC\": dec_data, \"redshift\": z_data})\n",
        "  if MAX_ROWS_DESI is not None and len(df) > MAX_ROWS_DESI:\n",
        "      df = df.iloc[:MAX_ROWS_DESI]\n",
        "\n",
        "  df.to_csv(output_path, index=False)\n",
        "  hdul.close()\n",
        "  print(f\"DESI: сохранено объектов: {len(df)}\")\n",
        "  return True\n",
        "\n",
        "def get_jwst_data():\n",
        "  ceers_url = \"https://web.corral.tacc.utexas.edu/ceersdata/DR06/MIRI/miri_catalog.dat\"\n",
        "  \"\"\"Загрузить каталог JWST CEERS PR1 (MIRI) и сохранить в CSV.\"\"\"\n",
        "  output_path = os.path.join(OUTPUT_DIR, \"jwst.csv\")\n",
        "  if os.path.exists(output_path):\n",
        "      print(\"Файл JWST (jwst.csv) уже существует, пропуск загрузки.\")\n",
        "      return output_path\n",
        "  print(\"Скачивание данных JWST CEERS PR1 MIRI...\")\n",
        "  # Загружаем текстовый .dat файл\n",
        "  response = requests.get(JWST_URL)\n",
        "  response.raise_for_status()\n",
        "  content = response.text\n",
        "  # Читаем содержимое как таблицу ASCII с помощью astropy\n",
        "  table = ascii.read(content)\n",
        "  # Имена колонок (может быть 'RA' и 'Dec' или подобные)\n",
        "  col_names = table.colnames\n",
        "  ra_col  = find_column_name(col_names, [\"RA\", \"RAJ2000\", \"ALPHA_J2000\"])\n",
        "  dec_col = find_column_name(col_names, [\"Dec\", \"DEC\", \"DEJ2000\", \"Delta_J2000\"])\n",
        "  z_col   = find_column_name(col_names, [\"Z\", \"REDSHIFT\", \"Z_SPEC\", \"Z_PHOT\", \"PHOTOZ\"])\n",
        "  ra_data = table[ra_col]\n",
        "  dec_data = table[dec_col]\n",
        "  if z_col:\n",
        "      z_data = table[z_col]\n",
        "  else:\n",
        "      z_data = np.full(len(ra_data), np.nan)\n",
        "  # Конвертируем в DataFrame\n",
        "  df = pd.DataFrame({ \"RA\": np.array(ra_data), \"DEC\": np.array(dec_data), \"redshift\": np.array(z_data) })\n",
        "  if MAX_ROWS_JWST is not None and len(df) > MAX_ROWS_JWST:\n",
        "      df = df.iloc[:MAX_ROWS_JWST]\n",
        "  df.to_csv(output_path, index=False)\n",
        "  print(f\"JWST: сохранено объектов: {len(df)}\")\n",
        "  return True\n",
        "\n",
        "\n",
        "def get_des_data():\n",
        "    DES_URL    = \"http://desdr-server.ncsa.illinois.edu/despublic/Y6_GOLD_v2.0.fits\"\n",
        "    \"\"\"Загрузить каталог DES Y6 GOLD и сохранить в CSV (построчная обработка из-за большого объема).\"\"\"\n",
        "    output_path = os.path.join(OUTPUT_DIR, \"des.csv\")\n",
        "    if os.path.exists(output_path):\n",
        "        print(\"Файл DES (des.csv) уже существует, пропуск загрузки.\")\n",
        "        return output_path\n",
        "    print(\"Скачивание данных DES Y6 GOLD (может занять время)...\")\n",
        "    hdul = fits.open(DES_URL, memmap=True)\n",
        "    data_hdu = hdul[1]\n",
        "    # Определяем имена колонок\n",
        "    ra_col  = find_column_name(data_hdu.columns.names, [\"RA\", \"RAJ2000\", \"ALPHA_J2000\"])\n",
        "    dec_col = find_column_name(data_hdu.columns.names, [\"DEC\", \"DECJ2000\", \"DEJ2000\", \"DELTA_J2000\"])\n",
        "    z_col   = find_column_name(data_hdu.columns.names, [\"Z\", \"REDSHIFT\", \"PHOTOZ\", \"Z_MEAN\", \"Z_SPEC\", \"ZPHOT\"])\n",
        "    # Получаем общее число строк и применяем ограничение MAX_ROWS_DES\n",
        "    total_rows = data_hdu.header.get('NAXIS2', None)\n",
        "    if total_rows is None:\n",
        "        total_rows = len(data_hdu.data)  # на случай, если NAXIS2 недоступен\n",
        "    if MAX_ROWS_DES is not None and total_rows > MAX_ROWS_DES:\n",
        "        target_rows = MAX_ROWS_DES\n",
        "    else:\n",
        "        target_rows = total_rows\n",
        "    # Открываем выходной файл и пишем заголовок\n",
        "    with open(output_path, 'w', encoding='utf-8') as f_out:\n",
        "        f_out.write(\"RA,DEC,redshift\\n\")\n",
        "    # Читаем и сохраняем данные чанками, чтобы не загружать все в память сразу\n",
        "    rows_processed = 0\n",
        "    for start in range(0, target_rows, CHUNK_SIZE):\n",
        "        stop = min(target_rows, start + CHUNK_SIZE)\n",
        "        data_chunk = data_hdu.data[start:stop]  # извлекаем срез данных\n",
        "        ra_data = data_chunk[ra_col]\n",
        "        dec_data = data_chunk[dec_col]\n",
        "        if z_col:\n",
        "            z_data = data_chunk[z_col]\n",
        "        else:\n",
        "            z_data = np.full(len(ra_data), np.nan)\n",
        "        # Обрабатываем маски при наличии\n",
        "        if hasattr(ra_data, 'mask'):\n",
        "            ra_data = ra_data.filled(np.nan)\n",
        "        if hasattr(dec_data, 'mask'):\n",
        "            dec_data = dec_data.filled(np.nan)\n",
        "        if hasattr(z_data, 'mask'):\n",
        "            z_data = z_data.filled(np.nan)\n",
        "        # Формируем DataFrame для чанка и дописываем в CSV\n",
        "        chunk_df = pd.DataFrame({\"RA\": ra_data, \"DEC\": dec_data, \"redshift\": z_data})\n",
        "        # Режим 'a' добавляет строки, header=False чтобы не дублировать заголовок\n",
        "        chunk_df.to_csv(output_path, mode='a', header=False, index=False)\n",
        "        rows_processed += len(chunk_df)\n",
        "        print(f\"DES: обработано {rows_processed} из ~{target_rows} объектов...\", end=\"\\r\")\n",
        "    hdul.close()\n",
        "    print(f\"\\nDES: сохранено объектов: {rows_processed}\")\n",
        "    return True\n",
        "\n",
        "def merge_all_data():\n",
        "    \"\"\"\n",
        "    Объединить все каталоги в единый набор и преобразовать сферические координаты (RA, DEC, redshift) в декартовые (X, Y, Z).\n",
        "    \"\"\"\n",
        "    # Пути ко всем подготовленным CSV-файлам\n",
        "    data_files = [\n",
        "        (os.path.join(OUTPUT_DIR, \"euclid.csv\"), \"Euclid\"),\n",
        "        (os.path.join(OUTPUT_DIR, \"desi.csv\"),   \"DESI\"),\n",
        "        (os.path.join(OUTPUT_DIR, \"sdss.csv\"),   \"SDSS\"),\n",
        "        (os.path.join(OUTPUT_DIR, \"jwst.csv\"),   \"JWST\"),\n",
        "        (os.path.join(OUTPUT_DIR, \"des.csv\"),    \"DES\")\n",
        "    ]\n",
        "    # Открываем выходной файл для объединенных данных\n",
        "    merged_path = os.path.join(OUTPUT_DIR, \"all_data.csv\")\n",
        "    with open(merged_path, 'w', encoding='utf-8') as f_out:\n",
        "        f_out.write(\"survey,RA,DEC,redshift,X,Y,Z\\n\")\n",
        "    total_count = 0\n",
        "    # Обрабатываем каждый каталог по очереди, чтобы не хранить все данные одновременно в памяти\n",
        "    for file_path, survey_name in data_files:\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"Предупреждение: файл {file_path} не найден, пропуск.\")\n",
        "            continue\n",
        "        print(f\"Объединение данных: {survey_name}\")\n",
        "        # Читаем входной CSV чанками, чтобы обработка была поэтапной\n",
        "        for chunk in pd.read_csv(file_path, chunksize=CHUNK_SIZE):\n",
        "            # Добавляем колонку с названием обзора\n",
        "            chunk.insert(0, 'survey', survey_name)\n",
        "            # Переводим координаты в радианы\n",
        "            ra_rad = np.deg2rad(chunk[\"RA\"].values)\n",
        "            dec_rad = np.deg2rad(chunk[\"DEC\"].values)\n",
        "            z_vals = chunk[\"redshift\"].values\n",
        "            # Вычисляем комовскую дистанцию (в Mpc) для каждого redshift (для NaN останется NaN)\n",
        "            distances = np.full(z_vals.shape, np.nan)\n",
        "            mask = np.isfinite(z_vals)  # маска для имеющих определённый z\n",
        "            if mask.any():\n",
        "                distances[mask] = cosmo.comoving_distance(z_vals[mask]).to(u.Mpc).value\n",
        "            # Расчет декартовых координат\n",
        "            x = distances * np.cos(dec_rad) * np.cos(ra_rad)\n",
        "            y = distances * np.cos(dec_rad) * np.sin(ra_rad)\n",
        "            z_cart = distances * np.sin(dec_rad)\n",
        "            # Добавляем колонки X, Y, Z в chunk\n",
        "            chunk[\"X\"] = x\n",
        "            chunk[\"Y\"] = y\n",
        "            chunk[\"Z\"] = z_cart\n",
        "            # Дописываем chunk в объединенный CSV\n",
        "            chunk.to_csv(merged_path, mode='a', header=False, index=False)\n",
        "            total_count += len(chunk)\n",
        "    print(f\"Объединение завершено. Общее число объектов: {total_count}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Главная функция скрипта: последовательно получает и объединяет все данные\n",
        "    \"\"\"\n",
        "    logger.info(\"=== Начало сбора данных галактик ===\")\n",
        "\n",
        "    # Получение данных из всех источников\n",
        "    success = True\n",
        "    success = get_euclid_data()\n",
        "    success = get_sdss_data()\n",
        "    success = get_desi_data()\n",
        "    success = get_jwst_data()\n",
        "\n",
        "    if not success:\n",
        "        logger.warning(\"Некоторые источники данных не были получены.\")\n",
        "\n",
        "    # Объединение и преобразование\n",
        "    if merge_all_data():\n",
        "        logger.info(\"=== Завершено успешно: все данные объединены и обработаны ===\")\n",
        "    else:\n",
        "        logger.error(\"Произошла ошибка при объединении данных.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 597
        },
        "id": "nVV_BhHAvuot",
        "outputId": "35601746-e759-44d9-f9a0-660856ba1ca0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Euclid: поиск HDU и чтение таблицы...\n",
            "  → HDU=1, строк=465\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "Euclid: не найдены RA/DEC в ['OBJECT_ID', 'RIGHT_ASCENSION', 'DECLINATION', 'RIGHT_ASCENSION_PSF_FITTING', 'DECLINATION_PSF_FITTING', 'SEGMENTATION_MAP_ID', 'VIS_DET', 'FLUX_VIS_1FWHM_APER', 'FLUX_VIS_2FWHM_APER', 'FLUX_VIS_3FWHM_APER', 'FLUX_VIS_4FWHM_APER', 'FLUX_Y_1FWHM_APER', 'FLUX_Y_2FWHM_APER', 'FLUX_Y_3FWHM_APER', 'FLUX_Y_4FWHM_APER', 'FLUX_J_1FWHM_APER', 'FLUX_J_2FWHM_APER', 'FLUX_J_3FWHM_APER', 'FLUX_J_4FWHM_APER', 'FLUX_H_1FWHM_APER', 'FLUX_H_2FWHM_APER', 'FLUX_H_3FWHM_APER', 'FLUX_H_4FWHM_APER', 'FLUX_NIR_STACK_1FWHM_APER', 'FLUX_NIR_STACK_2FWHM_APER', 'FLUX_NIR_STACK_3FWHM_APER', 'FLUX_NIR_STACK_4FWHM_APER', 'FLUX_U_EXT_DECAM_1FWHM_APER', 'FLUX_U_EXT_DECAM_2FWHM_APER', 'FLUX_U_EXT_DECAM_3FWHM_APER', 'FLUX_U_EXT_DECAM_4FWHM_APER', 'FLUX_G_EXT_DECAM_1FWHM_APER', 'FLUX_G_EXT_DECAM_2FWHM_APER', 'FLUX_G_EXT_DECAM_3FWHM_APER', 'FLUX_G_EXT_DECAM_4FWHM_APER', 'FLUX_R_EXT_DECAM_1FWHM_APER', 'FLUX_R_EXT_DECAM_2FWHM_APER', 'FLUX_R_EXT_DECAM_3FWHM_APER', 'FLUX_R_EXT_DECAM_4FWHM_APER', 'FLUX_I_EXT_DECAM_1FWHM_APER', 'FLUX_I_EXT_DECAM_2FWHM_APER', 'FLUX_I_EXT_DECAM_3FWHM_APER', 'FLUX_I_EXT_DECAM_4FWHM_APER', 'FLUX_Z_EXT_DECAM_1FWHM_APER', 'FLUX_Z_EXT_DECAM_2FWHM_APER', 'FLUX_Z_EXT_DECAM_3FWHM_APER', 'FLUX_Z_EXT_DECAM_4FWHM_APER', 'FLUX_U_EXT_LSST_1FWHM_APER', 'FLUX_U_EXT_LSST_2FWHM_APER', 'FLUX_U_EXT_LSST_3FWHM_APER', 'FLUX_U_EXT_LSST_4FWHM_APER', 'FLUX_G_EXT_LSST_1FWHM_APER', 'FLUX_G_EXT_LSST_2FWHM_APER', 'FLUX_G_EXT_LSST_3FWHM_APER', 'FLUX_G_EXT_LSST_4FWHM_APER', 'FLUX_R_EXT_LSST_1FWHM_APER', 'FLUX_R_EXT_LSST_2FWHM_APER', 'FLUX_R_EXT_LSST_3FWHM_APER', 'FLUX_R_EXT_LSST_4FWHM_APER', 'FLUX_I_EXT_LSST_1FWHM_APER', 'FLUX_I_EXT_LSST_2FWHM_APER', 'FLUX_I_EXT_LSST_3FWHM_APER', 'FLUX_I_EXT_LSST_4FWHM_APER', 'FLUX_Z_EXT_LSST_1FWHM_APER', 'FLUX_Z_EXT_LSST_2FWHM_APER', 'FLUX_Z_EXT_LSST_3FWHM_APER', 'FLUX_Z_EXT_LSST_4FWHM_APER', 'FLUX_U_EXT_MEGACAM_1FWHM_APER', 'FLUX_U_EXT_MEGACAM_2FWHM_APER', 'FLUX_U_EXT_MEGACAM_3FWHM_APER', 'FLUX_U_EXT_MEGACAM_4FWHM_APER', 'FLUX_R_EXT_MEGACAM_1FWHM_APER', 'FLUX_R_EXT_MEGACAM_2FWHM_APER', 'FLUX_R_EXT_MEGACAM_3FWHM_APER', 'FLUX_R_EXT_MEGACAM_4FWHM_APER', 'FLUX_G_EXT_JPCAM_1FWHM_APER', 'FLUX_G_EXT_JPCAM_2FWHM_APER', 'FLUX_G_EXT_JPCAM_3FWHM_APER', 'FLUX_G_EXT_JPCAM_4FWHM_APER', 'FLUX_I_EXT_PANSTARRS_1FWHM_APER', 'FLUX_I_EXT_PANSTARRS_2FWHM_APER', 'FLUX_I_EXT_PANSTARRS_3FWHM_APER', 'FLUX_I_EXT_PANSTARRS_4FWHM_APER', 'FLUX_Z_EXT_PANSTARRS_1FWHM_APER', 'FLUX_Z_EXT_PANSTARRS_2FWHM_APER', 'FLUX_Z_EXT_PANSTARRS_3FWHM_APER', 'FLUX_Z_EXT_PANSTARRS_4FWHM_APER', 'FLUX_G_EXT_HSC_1FWHM_APER', 'FLUX_G_EXT_HSC_2FWHM_APER', 'FLUX_G_EXT_HSC_3FWHM_APER', 'FLUX_G_EXT_HSC_4FWHM_APER', 'FLUX_Z_EXT_HSC_1FWHM_APER', 'FLUX_Z_EXT_HSC_2FWHM_APER', 'FLUX_Z_EXT_HSC_3FWHM_APER', 'FLUX_Z_EXT_HSC_4FWHM_APER', 'FLUXERR_VIS_1FWHM_APER', 'FLUXERR_VIS_2FWHM_APER', 'FLUXERR_VIS_3FWHM_APER', 'FLUXERR_VIS_4FWHM_APER', 'FLUXERR_Y_1FWHM_APER', 'FLUXERR_Y_2FWHM_APER', 'FLUXERR_Y_3FWHM_APER', 'FLUXERR_Y_4FWHM_APER', 'FLUXERR_J_1FWHM_APER', 'FLUXERR_J_2FWHM_APER', 'FLUXERR_J_3FWHM_APER', 'FLUXERR_J_4FWHM_APER', 'FLUXERR_H_1FWHM_APER', 'FLUXERR_H_2FWHM_APER', 'FLUXERR_H_3FWHM_APER', 'FLUXERR_H_4FWHM_APER', 'FLUXERR_NIR_STACK_1FWHM_APER', 'FLUXERR_NIR_STACK_2FWHM_APER', 'FLUXERR_NIR_STACK_3FWHM_APER', 'FLUXERR_NIR_STACK_4FWHM_APER', 'FLUXERR_U_EXT_DECAM_1FWHM_APER', 'FLUXERR_U_EXT_DECAM_2FWHM_APER', 'FLUXERR_U_EXT_DECAM_3FWHM_APER', 'FLUXERR_U_EXT_DECAM_4FWHM_APER', 'FLUXERR_G_EXT_DECAM_1FWHM_APER', 'FLUXERR_G_EXT_DECAM_2FWHM_APER', 'FLUXERR_G_EXT_DECAM_3FWHM_APER', 'FLUXERR_G_EXT_DECAM_4FWHM_APER', 'FLUXERR_R_EXT_DECAM_1FWHM_APER', 'FLUXERR_R_EXT_DECAM_2FWHM_APER', 'FLUXERR_R_EXT_DECAM_3FWHM_APER', 'FLUXERR_R_EXT_DECAM_4FWHM_APER', 'FLUXERR_I_EXT_DECAM_1FWHM_APER', 'FLUXERR_I_EXT_DECAM_2FWHM_APER', 'FLUXERR_I_EXT_DECAM_3FWHM_APER', 'FLUXERR_I_EXT_DECAM_4FWHM_APER', 'FLUXERR_Z_EXT_DECAM_1FWHM_APER', 'FLUXERR_Z_EXT_DECAM_2FWHM_APER', 'FLUXERR_Z_EXT_DECAM_3FWHM_APER', 'FLUXERR_Z_EXT_DECAM_4FWHM_APER', 'FLUXERR_U_EXT_LSST_1FWHM_APER', 'FLUXERR_U_EXT_LSST_2FWHM_APER', 'FLUXERR_U_EXT_LSST_3FWHM_APER', 'FLUXERR_U_EXT_LSST_4FWHM_APER', 'FLUXERR_G_EXT_LSST_1FWHM_APER', 'FLUXERR_G_EXT_LSST_2FWHM_APER', 'FLUXERR_G_EXT_LSST_3FWHM_APER', 'FLUXERR_G_EXT_LSST_4FWHM_APER', 'FLUXERR_R_EXT_LSST_1FWHM_APER', 'FLUXERR_R_EXT_LSST_2FWHM_APER', 'FLUXERR_R_EXT_LSST_3FWHM_APER', 'FLUXERR_R_EXT_LSST_4FWHM_APER', 'FLUXERR_I_EXT_LSST_1FWHM_APER', 'FLUXERR_I_EXT_LSST_2FWHM_APER', 'FLUXERR_I_EXT_LSST_3FWHM_APER', 'FLUXERR_I_EXT_LSST_4FWHM_APER', 'FLUXERR_Z_EXT_LSST_1FWHM_APER', 'FLUXERR_Z_EXT_LSST_2FWHM_APER', 'FLUXERR_Z_EXT_LSST_3FWHM_APER', 'FLUXERR_Z_EXT_LSST_4FWHM_APER', 'FLUXERR_U_EXT_MEGACAM_1FWHM_APER', 'FLUXERR_U_EXT_MEGACAM_2FWHM_APER', 'FLUXERR_U_EXT_MEGACAM_3FWHM_APER', 'FLUXERR_U_EXT_MEGACAM_4FWHM_APER', 'FLUXERR_R_EXT_MEGACAM_1FWHM_APER', 'FLUXERR_R_EXT_MEGACAM_2FWHM_APER', 'FLUXERR_R_EXT_MEGACAM_3FWHM_APER', 'FLUXERR_R_EXT_MEGACAM_4FWHM_APER', 'FLUXERR_G_EXT_JPCAM_1FWHM_APER', 'FLUXERR_G_EXT_JPCAM_2FWHM_APER', 'FLUXERR_G_EXT_JPCAM_3FWHM_APER', 'FLUXERR_G_EXT_JPCAM_4FWHM_APER', 'FLUXERR_I_EXT_PANSTARRS_1FWHM_APER', 'FLUXERR_I_EXT_PANSTARRS_2FWHM_APER', 'FLUXERR_I_EXT_PANSTARRS_3FWHM_APER', 'FLUXERR_I_EXT_PANSTARRS_4FWHM_APER', 'FLUXERR_Z_EXT_PANSTARRS_1FWHM_APER', 'FLUXERR_Z_EXT_PANSTARRS_2FWHM_APER', 'FLUXERR_Z_EXT_PANSTARRS_3FWHM_APER', 'FLUXERR_Z_EXT_PANSTARRS_4FWHM_APER', 'FLUXERR_G_EXT_HSC_1FWHM_APER', 'FLUXERR_G_EXT_HSC_2FWHM_APER', 'FLUXERR_G_EXT_HSC_3FWHM_APER', 'FLUXERR_G_EXT_HSC_4FWHM_APER', 'FLUXERR_Z_EXT_HSC_1FWHM_APER', 'FLUXERR_Z_EXT_HSC_2FWHM_APER', 'FLUXERR_Z_EXT_HSC_3FWHM_APER', 'FLUXERR_Z_EXT_HSC_4FWHM_APER', 'FLUX_Y_TEMPLFIT', 'FLUX_J_TEMPLFIT', 'FLUX_H_TEMPLFIT', 'FLUX_U_EXT_DECAM_TEMPLFIT', 'FLUX_G_EXT_DECAM_TEMPLFIT', 'FLUX_R_EXT_DECAM_TEMPLFIT', 'FLUX_I_EXT_DECAM_TEMPLFIT', 'FLUX_Z_EXT_DECAM_TEMPLFIT', 'FLUX_U_EXT_LSST_TEMPLFIT', 'FLUX_G_EXT_LSST_TEMPLFIT', 'FLUX_R_EXT_LSST_TEMPLFIT', 'FLUX_I_EXT_LSST_TEMPLFIT', 'FLUX_Z_EXT_LSST_TEMPLFIT', 'FLUX_U_EXT_MEGACAM_TEMPLFIT', 'FLUX_R_EXT_MEGACAM_TEMPLFIT', 'FLUX_G_EXT_JPCAM_TEMPLFIT', 'FLUX_I_EXT_PANSTARRS_TEMPLFIT', 'FLUX_Z_EXT_PANSTARRS_TEMPLFIT', 'FLUX_G_EXT_HSC_TEMPLFIT', 'FLUX_Z_EXT_HSC_TEMPLFIT', 'FLUXERR_Y_TEMPLFIT', 'FLUXERR_J_TEMPLFIT', 'FLUXERR_H_TEMPLFIT', 'FLUXERR_U_EXT_DECAM_TEMPLFIT', 'FLUXERR_G_EXT_DECAM_TEMPLFIT', 'FLUXERR_R_EXT_DECAM_TEMPLFIT', 'FLUXERR_I_EXT_DECAM_TEMPLFIT', 'FLUXERR_Z_EXT_DECAM_TEMPLFIT', 'FLUXERR_U_EXT_LSST_TEMPLFIT', 'FLUXERR_G_EXT_LSST_TEMPLFIT', 'FLUXERR_R_EXT_LSST_TEMPLFIT', 'FLUXERR_I_EXT_LSST_TEMPLFIT', 'FLUXERR_Z_EXT_LSST_TEMPLFIT', 'FLUXERR_U_EXT_MEGACAM_TEMPLFIT', 'FLUXERR_R_EXT_MEGACAM_TEMPLFIT', 'FLUXERR_G_EXT_JPCAM_TEMPLFIT', 'FLUXERR_I_EXT_PANSTARRS_TEMPLFIT', 'FLUXERR_Z_EXT_PANSTARRS_TEMPLFIT', 'FLUXERR_G_EXT_HSC_TEMPLFIT', 'FLUXERR_Z_EXT_HSC_TEMPLFIT', 'FLUX_VIS_TO_Y_TEMPLFIT', 'FLUX_VIS_TO_J_TEMPLFIT', 'FLUX_VIS_TO_H_TEMPLFIT', 'FLUX_VIS_TO_U_EXT_DECAM_TEMPLFIT', 'FLUX_VIS_TO_G_EXT_DECAM_TEMPLFIT', 'FLUX_VIS_TO_R_EXT_DECAM_TEMPLFIT', 'FLUX_VIS_TO_I_EXT_DECAM_TEMPLFIT', 'FLUX_VIS_TO_Z_EXT_DECAM_TEMPLFIT', 'FLUX_VIS_TO_U_EXT_LSST_TEMPLFIT', 'FLUX_VIS_TO_G_EXT_LSST_TEMPLFIT', 'FLUX_VIS_TO_R_EXT_LSST_TEMPLFIT', 'FLUX_VIS_TO_I_EXT_LSST_TEMPLFIT', 'FLUX_VIS_TO_Z_EXT_LSST_TEMPLFIT', 'FLUX_VIS_TO_U_EXT_MEGACAM_TEMPLFIT', 'FLUX_VIS_TO_R_EXT_MEGACAM_TEMPLFIT', 'FLUX_VIS_TO_G_EXT_JPCAM_TEMPLFIT', 'FLUX_VIS_TO_I_EXT_PANSTARRS_TEMPLFIT', 'FLUX_VIS_TO_Z_EXT_PANSTARRS_TEMPLFIT', 'FLUX_VIS_TO_G_EXT_HSC_TEMPLFIT', 'FLUX_VIS_TO_Z_EXT_HSC_TEMPLFIT', 'FLUXERR_VIS_TO_Y_TEMPLFIT', 'FLUXERR_VIS_TO_J_TEMPLFIT', 'FLUXERR_VIS_TO_H_TEMPLFIT', 'FLUXERR_VIS_TO_U_EXT_DECAM_TEMPLFIT', 'FLUXERR_VIS_TO_G_EXT_DECAM_TEMPLFIT', 'FLUXERR_VIS_TO_R_EXT_DECAM_TEMPLFIT', 'FLUXERR_VIS_TO_I_EXT_DECAM_TEMPLFIT', 'FLUXERR_VIS_TO_Z_EXT_DECAM_TEMPLFIT', 'FLUXERR_VIS_TO_U_EXT_LSST_TEMPLFIT', 'FLUXERR_VIS_TO_G_EXT_LSST_TEMPLFIT', 'FLUXERR_VIS_TO_R_EXT_LSST_TEMPLFIT', 'FLUXERR_VIS_TO_I_EXT_LSST_TEMPLFIT', 'FLUXERR_VIS_TO_Z_EXT_LSST_TEMPLFIT', 'FLUXERR_VIS_TO_U_EXT_MEGACAM_TEMPLFIT', 'FLUXERR_VIS_TO_R_EXT_MEGACAM_TEMPLFIT', 'FLUXERR_VIS_TO_G_EXT_JPCAM_TEMPLFIT', 'FLUXERR_VIS_TO_I_EXT_PANSTARRS_TEMPLFIT', 'FLUXERR_VIS_TO_Z_EXT_PANSTARRS_TEMPLFIT', 'FLUXERR_VIS_TO_G_EXT_HSC_TEMPLFIT', 'FLUXERR_VIS_TO_Z_EXT_HSC_TEMPLFIT', 'FLUX_VIS_PSF', 'FLUXERR_VIS_PSF', 'FLUX_SEGMENTATION', 'FLUXERR_SEGMENTATION', 'FLUX_DETECTION_TOTAL', 'FLUXERR_DETECTION_TOTAL', 'FLUX_VIS_SERSIC', 'FLUX_Y_SERSIC', 'FLUX_J_SERSIC', 'FLUX_H_SERSIC', 'FLUX_U_EXT_DECAM_SERSIC', 'FLUX_G_EXT_DECAM_SERSIC', 'FLUX_R_EXT_DECAM_SERSIC', 'FLUX_I_EXT_DECAM_SERSIC', 'FLUX_Z_EXT_DECAM_SERSIC', 'FLUX_U_EXT_LSST_SERSIC', 'FLUX_G_EXT_LSST_SERSIC', 'FLUX_R_EXT_LSST_SERSIC', 'FLUX_I_EXT_LSST_SERSIC', 'FLUX_Z_EXT_LSST_SERSIC', 'FLUX_U_EXT_MEGACAM_SERSIC', 'FLUX_R_EXT_MEGACAM_SERSIC', 'FLUX_G_EXT_JPCAM_SERSIC', 'FLUX_I_EXT_PANSTARRS_SERSIC', 'FLUX_Z_EXT_PANSTARRS_SERSIC', 'FLUX_G_EXT_HSC_SERSIC', 'FLUX_Z_EXT_HSC_SERSIC', 'FLUXERR_VIS_SERSIC', 'FLUXERR_Y_SERSIC', 'FLUXERR_J_SERSIC', 'FLUXERR_H_SERSIC', 'FLUXERR_U_EXT_DECAM_SERSIC', 'FLUXERR_G_EXT_DECAM_SERSIC', 'FLUXERR_R_EXT_DECAM_SERSIC', 'FLUXERR_I_EXT_DECAM_SERSIC', 'FLUXERR_Z_EXT_DECAM_SERSIC', 'FLUXERR_U_EXT_LSST_SERSIC', 'FLUXERR_G_EXT_LSST_SERSIC', 'FLUXERR_R_EXT_LSST_SERSIC', 'FLUXERR_I_EXT_LSST_SERSIC', 'FLUXERR_Z_EXT_LSST_SERSIC', 'FLUXERR_U_EXT_MEGACAM_SERSIC', 'FLUXERR_R_EXT_MEGACAM_SERSIC', 'FLUXERR_G_EXT_JPCAM_SERSIC', 'FLUXERR_I_EXT_PANSTARRS_SERSIC', 'FLUXERR_Z_EXT_PANSTARRS_SERSIC', 'FLUXERR_G_EXT_HSC_SERSIC', 'FLUXERR_Z_EXT_HSC_SERSIC', 'FLUX_VIS_DISK_SERSIC', 'FLUX_Y_DISK_SERSIC', 'FLUX_J_DISK_SERSIC', 'FLUX_H_DISK_SERSIC', 'FLUX_U_EXT_DECAM_DISK_SERSIC', 'FLUX_G_EXT_DECAM_DISK_SERSIC', 'FLUX_R_EXT_DECAM_DISK_SERSIC', 'FLUX_I_EXT_DECAM_DISK_SERSIC', 'FLUX_Z_EXT_DECAM_DISK_SERSIC', 'FLUX_U_EXT_LSST_DISK_SERSIC', 'FLUX_G_EXT_LSST_DISK_SERSIC', 'FLUX_R_EXT_LSST_DISK_SERSIC', 'FLUX_I_EXT_LSST_DISK_SERSIC', 'FLUX_Z_EXT_LSST_DISK_SERSIC', 'FLUX_U_EXT_MEGACAM_DISK_SERSIC', 'FLUX_R_EXT_MEGACAM_DISK_SERSIC', 'FLUX_G_EXT_JPCAM_DISK_SERSIC', 'FLUX_I_EXT_PANSTARRS_DISK_SERSIC', 'FLUX_Z_EXT_PANSTARRS_DISK_SERSIC', 'FLUX_G_EXT_HSC_DISK_SERSIC', 'FLUX_Z_EXT_HSC_DISK_SERSIC', 'FLUXERR_VIS_DISK_SERSIC', 'FLUXERR_Y_DISK_SERSIC', 'FLUXERR_J_DISK_SERSIC', 'FLUXERR_H_DISK_SERSIC', 'FLUXERR_U_EXT_DECAM_DISK_SERSIC', 'FLUXERR_G_EXT_DECAM_DISK_SERSIC', 'FLUXERR_R_EXT_DECAM_DISK_SERSIC', 'FLUXERR_I_EXT_DECAM_DISK_SERSIC', 'FLUXERR_Z_EXT_DECAM_DISK_SERSIC', 'FLUXERR_U_EXT_LSST_DISK_SERSIC', 'FLUXERR_G_EXT_LSST_DISK_SERSIC', 'FLUXERR_R_EXT_LSST_DISK_SERSIC', 'FLUXERR_I_EXT_LSST_DISK_SERSIC', 'FLUXERR_Z_EXT_LSST_DISK_SERSIC', 'FLUXERR_U_EXT_MEGACAM_DISK_SERSIC', 'FLUXERR_R_EXT_MEGACAM_DISK_SERSIC', 'FLUXERR_G_EXT_JPCAM_DISK_SERSIC', 'FLUXERR_I_EXT_PANSTARRS_DISK_SERSIC', 'FLUXERR_Z_EXT_PANSTARRS_DISK_SERSIC', 'FLUXERR_G_EXT_HSC_DISK_SERSIC', 'FLUXERR_Z_EXT_HSC_DISK_SERSIC', 'SERSIC_FRACT_VIS_DISK_SERSIC', 'SERSIC_FRACT_Y_DISK_SERSIC', 'SERSIC_FRACT_J_DISK_SERSIC', 'SERSIC_FRACT_H_DISK_SERSIC', 'SERSIC_FRACT_U_EXT_DECAM_DISK_SERSIC', 'SERSIC_FRACT_G_EXT_DECAM_DISK_SERSIC', 'SERSIC_FRACT_R_EXT_DECAM_DISK_SERSIC', 'SERSIC_FRACT_I_EXT_DECAM_DISK_SERSIC', 'SERSIC_FRACT_Z_EXT_DECAM_DISK_SERSIC', 'SERSIC_FRACT_U_EXT_LSST_DISK_SERSIC', 'SERSIC_FRACT_G_EXT_LSST_DISK_SERSIC', 'SERSIC_FRACT_R_EXT_LSST_DISK_SERSIC', 'SERSIC_FRACT_I_EXT_LSST_DISK_SERSIC', 'SERSIC_FRACT_Z_EXT_LSST_DISK_SERSIC', 'SERSIC_FRACT_U_EXT_MEGACAM_DISK_SERSIC', 'SERSIC_FRACT_R_EXT_MEGACAM_DISK_SERSIC', 'SERSIC_FRACT_G_EXT_JPCAM_DISK_SERSIC', 'SERSIC_FRACT_I_EXT_PANSTARRS_DISK_SERSIC', 'SERSIC_FRACT_Z_EXT_PANSTARRS_DISK_SERSIC', 'SERSIC_FRACT_G_EXT_HSC_DISK_SERSIC', 'SERSIC_FRACT_Z_EXT_HSC_DISK_SERSIC', 'SERSIC_FRACT_VIS_DISK_SERSIC_ERR', 'SERSIC_FRACT_Y_DISK_SERSIC_ERR', 'SERSIC_FRACT_J_DISK_SERSIC_ERR', 'SERSIC_FRACT_H_DISK_SERSIC_ERR', 'SERSIC_FRACT_U_EXT_DECAM_DISK_SERSIC_ERR', 'SERSIC_FRACT_G_EXT_DECAM_DISK_SERSIC_ERR', 'SERSIC_FRACT_R_EXT_DECAM_DISK_SERSIC_ERR', 'SERSIC_FRACT_I_EXT_DECAM_DISK_SERSIC_ERR', 'SERSIC_FRACT_Z_EXT_DECAM_DISK_SERSIC_ERR', 'SERSIC_FRACT_U_EXT_LSST_DISK_SERSIC_ERR', 'SERSIC_FRACT_G_EXT_LSST_DISK_SERSIC_ERR', 'SERSIC_FRACT_R_EXT_LSST_DISK_SERSIC_ERR', 'SERSIC_FRACT_I_EXT_LSST_DISK_SERSIC_ERR', 'SERSIC_FRACT_Z_EXT_LSST_DISK_SERSIC_ERR', 'SERSIC_FRACT_U_EXT_MEGACAM_DISK_SERSIC_ERR', 'SERSIC_FRACT_R_EXT_MEGACAM_DISK_SERSIC_ERR', 'SERSIC_FRACT_G_EXT_JPCAM_DISK_SERSIC_ERR', 'SERSIC_FRACT_I_EXT_PANSTARRS_DISK_SERSIC_ERR', 'SERSIC_FRACT_Z_EXT_PANSTARRS_DISK_SERSIC_ERR', 'SERSIC_FRACT_G_EXT_HSC_DISK_SERSIC_ERR', 'SERSIC_FRACT_Z_EXT_HSC_DISK_SERSIC_ERR', 'FLAG_VIS', 'FLAG_Y', 'FLAG_J', 'FLAG_H', 'FLAG_NIR_STACK', 'FLAG_U_EXT_DECAM', 'FLAG_G_EXT_DECAM', 'FLAG_R_EXT_DECAM', 'FLAG_I_EXT_DECAM', 'FLAG_Z_EXT_DECAM', 'FLAG_U_EXT_LSST', 'FLAG_G_EXT_LSST', 'FLAG_R_EXT_LSST', 'FLAG_I_EXT_LSST', 'FLAG_Z_EXT_LSST', 'FLAG_U_EXT_MEGACAM', 'FLAG_R_EXT_MEGACAM', 'FLAG_G_EXT_JPCAM', 'FLAG_I_EXT_PANSTARRS', 'FLAG_Z_EXT_PANSTARRS', 'FLAG_G_EXT_HSC', 'FLAG_Z_EXT_HSC', 'AVG_TRANS_WAVE_VIS', 'AVG_TRANS_WAVE_Y', 'AVG_TRANS_WAVE_J', 'AVG_TRANS_WAVE_H', 'AVG_TRANS_WAVE_U_EXT_DECAM', 'AVG_TRANS_WAVE_G_EXT_DECAM', 'AVG_TRANS_WAVE_R_EXT_DECAM', 'AVG_TRANS_WAVE_I_EXT_DECAM', 'AVG_TRANS_WAVE_Z_EXT_DECAM', 'AVG_TRANS_WAVE_U_EXT_LSST', 'AVG_TRANS_WAVE_G_EXT_LSST', 'AVG_TRANS_WAVE_R_EXT_LSST', 'AVG_TRANS_WAVE_I_EXT_LSST', 'AVG_TRANS_WAVE_Z_EXT_LSST', 'AVG_TRANS_WAVE_U_EXT_MEGACAM', 'AVG_TRANS_WAVE_R_EXT_MEGACAM', 'AVG_TRANS_WAVE_G_EXT_JPCAM', 'AVG_TRANS_WAVE_I_EXT_PANSTARRS', 'AVG_TRANS_WAVE_Z_EXT_PANSTARRS', 'AVG_TRANS_WAVE_G_EXT_HSC', 'AVG_TRANS_WAVE_Z_EXT_HSC', 'DEBLENDED_FLAG', 'PARENT_ID', 'PARENT_VISNIR', 'BLENDED_PROB', 'SHE_FLAG', 'VARIABLE_FLAG', 'BINARY_FLAG', 'POINT_LIKE_FLAG', 'POINT_LIKE_PROB', 'EXTENDED_FLAG', 'EXTENDED_PROB', 'SPURIOUS_FLAG', 'SPURIOUS_PROB', 'MAG_STARGAL_SEP', 'DET_QUALITY_FLAG', 'MU_MAX', 'MUMAX_MINUS_MAG', 'SEGMENTATION_AREA', 'SEMIMAJOR_AXIS', 'SEMIMAJOR_AXIS_ERR', 'POSITION_ANGLE', 'POSITION_ANGLE_ERR', 'ELLIPTICITY', 'ELLIPTICITY_ERR', 'KRON_RADIUS', 'KRON_RADIUS_ERR', 'FWHM', 'GAL_EBV', 'GAL_EBV_ERR', 'GAIA_ID', 'GAIA_MATCH_QUALITY']",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-c50db0fb396a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    610\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-3-c50db0fb396a>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    595\u001b[0m     \u001b[0;31m# Получение данных из всех источников\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m     \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 597\u001b[0;31m     \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_euclid_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    598\u001b[0m     \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_sdss_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m     \u001b[0msuccess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_desi_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-c50db0fb396a>\u001b[0m in \u001b[0;36mget_euclid_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m    354\u001b[0m     \u001b[0mz_col\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mfind_column_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m\"Z\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"REDSHIFT\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Z_SPEC\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"z_spec\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"PHOTOZ\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"redshift\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"z\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"z_phot\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Redshift\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mra_col\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdec_col\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Euclid: не найдены RA/DEC в {cols}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;31m# Красное смещение в MER Final обычно отсутствует — заполняем NaN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: Euclid: не найдены RA/DEC в ['OBJECT_ID', 'RIGHT_ASCENSION', 'DECLINATION', 'RIGHT_ASCENSION_PSF_FITTING', 'DECLINATION_PSF_FITTING', 'SEGMENTATION_MAP_ID', 'VIS_DET', 'FLUX_VIS_1FWHM_APER', 'FLUX_VIS_2FWHM_APER', 'FLUX_VIS_3FWHM_APER', 'FLUX_VIS_4FWHM_APER', 'FLUX_Y_1FWHM_APER', 'FLUX_Y_2FWHM_APER', 'FLUX_Y_3FWHM_APER', 'FLUX_Y_4FWHM_APER', 'FLUX_J_1FWHM_APER', 'FLUX_J_2FWHM_APER', 'FLUX_J_3FWHM_APER', 'FLUX_J_4FWHM_APER', 'FLUX_H_1FWHM_APER', 'FLUX_H_2FWHM_APER', 'FLUX_H_3FWHM_APER', 'FLUX_H_4FWHM_APER', 'FLUX_NIR_STACK_1FWHM_APER', 'FLUX_NIR_STACK_2FWHM_APER', 'FLUX_NIR_STACK_3FWHM_APER', 'FLUX_NIR_STACK_4FWHM_APER', 'FLUX_U_EXT_DECAM_1FWHM_APER', 'FLUX_U_EXT_DECAM_2FWHM_APER', 'FLUX_U_EXT_DECAM_3FWHM_APER', 'FLUX_U_EXT_DECAM_4FWHM_APER', 'FLUX_G_EXT_DECAM_1FWHM_APER', 'FLUX_G_EXT_DECAM_2FWHM_APER', 'FLUX_G_EXT_DECAM_3FWHM_APER', 'FLUX_G_EXT_DECAM_4FWHM_APER', 'FLUX_R_EXT_DECAM_1FWHM_APER', 'FLUX_R_EXT_DECAM_2FWHM_APER', 'FLUX_R_EXT_DECAM_3FWHM_APER', 'FLUX_R_EXT_DECAM_4FWHM_APER', 'FLUX_I_EXT_DECAM_1FWHM_APER', 'FLUX_I_EXT_DECAM_2FWHM_APER', 'FLUX_I_EXT_DECAM_3FWHM_APER', 'FLUX_I_EXT_DECAM_4FWHM_APER', 'FLUX_Z_EXT_DECAM_1FWHM_APER', 'FLUX_Z_EXT_DECAM_2FWHM_APER', 'FLUX_Z_EXT_DECAM_3FWHM_APER', 'FLUX_Z_EXT_DECAM_4FWHM_APER', 'FLUX_U_EXT_LSST_1FWHM_APER', 'FLUX_U_EXT_LSST_2FWHM_APER', 'FLUX_U_EXT_LSST_3FWHM_APER', 'FLUX_U_EXT_LSST_4FWHM_APER', 'FLUX_G_EXT_LSST_1FWHM_APER', 'FLUX_G_EXT_LSST_2FWHM_APER', 'FLUX_G_EXT_LSST_3FWHM_APER', 'FLUX_G_EXT_LS..."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Установка зависимостей и настройка окружения"
      ],
      "metadata": {
        "id": "vwUxAiocSopc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Установка зависимостей в Google Colab\n",
        "!pip install --quiet astroquery sdss-access pyvo astropy requests pyvo.dal astroquery.mast desilike\n",
        "\n",
        "#!pip install --quiet git+https://github.com/cosmodesi/desilike.git\n",
        "!pip install --upgrade --force-reinstall numpy==1.26.0 pandas==2.2.2 #right ver\n",
        "# 2. Импорт библиотек и настройка окружения\n",
        "import os\n",
        "import logging\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "from astropy import units as u\n",
        "from astropy.cosmology import Planck15\n",
        "from astropy.table import Table\n",
        "\n",
        "# SDSS через sdss-access\n",
        "from sdss_access import Access\n",
        "\n",
        "# DESI TAP-запросы\n",
        "from pyvo.dal import TAPService\n",
        "\n",
        "# JWST (если понадобится)\n",
        "from astroquery.mast import Observations\n",
        "\n",
        "# Euclid через IRSA\n",
        "from astroquery.ipac.irsa import Irsa\n",
        "\n",
        "# Настройка логирования\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s [%(levelname)s] %(message)s\")\n",
        "logger = logging.getLogger(__name__)\n",
        "logger.info(\"Окружение готово: зависимости установлены и библиотеки импортированы.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x-0v3oUvSs_V",
        "outputId": "bc581497-4489-4f85-fde9-8d6d1e22f40b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy==1.26.0 in /usr/local/lib/python3.11/dist-packages (1.26.0)\n",
            "Requirement already satisfied: pandas==2.2.2 in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas==2.2.2) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas==2.2.2) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Загрузка и фильтрация данных SDSS DR17"
      ],
      "metadata": {
        "id": "mVPRxagiRVdA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Источник:** SDSS DR17 specObj CSV (~2 GB .bz2)  \n",
        "**Задача:** оставить только внегалактические объекты (class≠STAR, zwarning=0) и сохранить столбцы ra, dec, z"
      ],
      "metadata": {
        "id": "maTvlA4vRloY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2.1. Скачиваем сжатый CSV\n",
        "sdss_url = \"https://dr17.sdss.org/sas/dr17/casload/spCSV/plates/sqlSpecObj.csv.bz2\"\n",
        "sdss_bz2 = \"sqlSpecObj_dr17.csv.bz2\"\n",
        "if not os.path.exists(sdss_bz2):\n",
        "    logger.info(\"Скачивание SDSS DR17...\")\n",
        "    r = requests.get(sdss_url, stream=True)\n",
        "    r.raise_for_status()\n",
        "    with open(sdss_bz2, \"wb\") as f:\n",
        "        for chunk in r.iter_content(8192):\n",
        "            f.write(chunk)\n",
        "    logger.info(\"Загрузка завершена.\")\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Просмотрим имена колонок в начале файла\n",
        "header = pd.read_csv(\"sqlSpecObj_dr17.csv.bz2\", compression=\"bz2\", nrows=0)\n",
        "print(\"Columns in SDSS CSV:\", header.columns.tolist())\n",
        "\n",
        "# 2.2. Фильтруем по чанкам\n",
        "sdss_out = \"sdss.csv\"\n",
        "with open(sdss_out, \"w\") as f:\n",
        "    f.write(\"ra,dec,z\\n\")\n",
        "\n",
        "cols = [\"ra\",\"dec\",\"z\",\"class\",\"zWarning\"]\n",
        "chunksize = 100_000\n",
        "total = 0\n",
        "for chunk in pd.read_csv(sdss_bz2, compression=\"bz2\", usecols=cols, chunksize=chunksize):\n",
        "    mask = (chunk[\"class\"].str.upper() != \"STAR\") & (chunk[\"zWarning\"] == 0)\n",
        "    df = chunk.loc[mask, [\"ra\",\"dec\",\"z\"]]\n",
        "    df.to_csv(sdss_out, mode=\"a\", header=False, index=False)\n",
        "    total += len(df)\n",
        "    logger.info(f\"SDSS: добавлено {len(df)} записей (итого {total})\")\n",
        "\n",
        "logger.info(f\"SDSS-фильтрация завершена. Всего объектов: {total}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mDz5BllIRr_-",
        "outputId": "c2bb8365-f348-4b71-b0d0-94ef5ece3171",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[0;31m[ERROR]: \u001b[0mTraceback (most recent call last):\n",
            "  File \u001b[36m\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\"\u001b[39;49;00m, line \u001b[34m3553\u001b[39;49;00m, in run_code\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00mexec(code_obj, \u001b[36mself\u001b[39;49;00m.user_global_ns, \u001b[36mself\u001b[39;49;00m.user_ns)\u001b[37m\u001b[39;49;00m\n",
            "  File \u001b[36m\"<ipython-input-3-6750489cf23f>\"\u001b[39;49;00m, line \u001b[34m9\u001b[39;49;00m, in <cell line: 0>\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m\u001b[34mfor\u001b[39;49;00m chunk \u001b[35min\u001b[39;49;00m r.iter_content(\u001b[34m8192\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
            "  File \u001b[36m\"/usr/local/lib/python3.11/dist-packages/requests/models.py\"\u001b[39;49;00m, line \u001b[34m820\u001b[39;49;00m, in generate\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m\u001b[34myield from\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.raw.stream(chunk_size, decode_content=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
            "  File \u001b[36m\"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\"\u001b[39;49;00m, line \u001b[34m1066\u001b[39;49;00m, in stream\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00mdata = \u001b[36mself\u001b[39;49;00m.read(amt=amt, decode_content=decode_content)\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m           \u001b[39;49;00m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[37m\u001b[39;49;00m\n",
            "  File \u001b[36m\"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\"\u001b[39;49;00m, line \u001b[34m955\u001b[39;49;00m, in read\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00mdata = \u001b[36mself\u001b[39;49;00m._raw_read(amt)\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m           \u001b[39;49;00m^^^^^^^^^^^^^^^^^^^\u001b[37m\u001b[39;49;00m\n",
            "  File \u001b[36m\"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\"\u001b[39;49;00m, line \u001b[34m879\u001b[39;49;00m, in _raw_read\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00mdata = \u001b[36mself\u001b[39;49;00m._fp_read(amt, read1=read1) \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m fp_closed \u001b[34melse\u001b[39;49;00m \u001b[33mb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m           \u001b[39;49;00m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[37m\u001b[39;49;00m\n",
            "  File \u001b[36m\"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\"\u001b[39;49;00m, line \u001b[34m862\u001b[39;49;00m, in _fp_read\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m\u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m._fp.read(amt) \u001b[34mif\u001b[39;49;00m amt \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m._fp.read()\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m           \u001b[39;49;00m^^^^^^^^^^^^^^^^^^\u001b[37m\u001b[39;49;00m\n",
            "  File \u001b[36m\"/usr/lib/python3.11/http/client.py\"\u001b[39;49;00m, line \u001b[34m473\u001b[39;49;00m, in read\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00ms = \u001b[36mself\u001b[39;49;00m.fp.read(amt)\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m^^^^^^^^^^^^^^^^^\u001b[37m\u001b[39;49;00m\n",
            "  File \u001b[36m\"/usr/lib/python3.11/socket.py\"\u001b[39;49;00m, line \u001b[34m718\u001b[39;49;00m, in readinto\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m\u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m._sock.recv_into(b)\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m           \u001b[39;49;00m^^^^^^^^^^^^^^^^^^^^^^^\u001b[37m\u001b[39;49;00m\n",
            "  File \u001b[36m\"/usr/lib/python3.11/ssl.py\"\u001b[39;49;00m, line \u001b[34m1314\u001b[39;49;00m, in recv_into\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m\u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.read(nbytes, buffer)\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m           \u001b[39;49;00m^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[37m\u001b[39;49;00m\n",
            "  File \u001b[36m\"/usr/lib/python3.11/ssl.py\"\u001b[39;49;00m, line \u001b[34m1166\u001b[39;49;00m, in read\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m\u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m._sslobj.read(\u001b[36mlen\u001b[39;49;00m, buffer)\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m           \u001b[39;49;00m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[37m\u001b[39;49;00m\n",
            "\u001b[91mKeyboardInterrupt\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
            "\n",
            "ERROR:sdss_access:Traceback (most recent call last):\n",
            "  File \u001b[36m\"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\"\u001b[39;49;00m, line \u001b[34m3553\u001b[39;49;00m, in run_code\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00mexec(code_obj, \u001b[36mself\u001b[39;49;00m.user_global_ns, \u001b[36mself\u001b[39;49;00m.user_ns)\u001b[37m\u001b[39;49;00m\n",
            "  File \u001b[36m\"<ipython-input-3-6750489cf23f>\"\u001b[39;49;00m, line \u001b[34m9\u001b[39;49;00m, in <cell line: 0>\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m\u001b[34mfor\u001b[39;49;00m chunk \u001b[35min\u001b[39;49;00m r.iter_content(\u001b[34m8192\u001b[39;49;00m):\u001b[37m\u001b[39;49;00m\n",
            "  File \u001b[36m\"/usr/local/lib/python3.11/dist-packages/requests/models.py\"\u001b[39;49;00m, line \u001b[34m820\u001b[39;49;00m, in generate\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m\u001b[34myield from\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.raw.stream(chunk_size, decode_content=\u001b[34mTrue\u001b[39;49;00m)\u001b[37m\u001b[39;49;00m\n",
            "  File \u001b[36m\"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\"\u001b[39;49;00m, line \u001b[34m1066\u001b[39;49;00m, in stream\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00mdata = \u001b[36mself\u001b[39;49;00m.read(amt=amt, decode_content=decode_content)\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m           \u001b[39;49;00m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[37m\u001b[39;49;00m\n",
            "  File \u001b[36m\"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\"\u001b[39;49;00m, line \u001b[34m955\u001b[39;49;00m, in read\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00mdata = \u001b[36mself\u001b[39;49;00m._raw_read(amt)\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m           \u001b[39;49;00m^^^^^^^^^^^^^^^^^^^\u001b[37m\u001b[39;49;00m\n",
            "  File \u001b[36m\"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\"\u001b[39;49;00m, line \u001b[34m879\u001b[39;49;00m, in _raw_read\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00mdata = \u001b[36mself\u001b[39;49;00m._fp_read(amt, read1=read1) \u001b[34mif\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m fp_closed \u001b[34melse\u001b[39;49;00m \u001b[33mb\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m           \u001b[39;49;00m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[37m\u001b[39;49;00m\n",
            "  File \u001b[36m\"/usr/local/lib/python3.11/dist-packages/urllib3/response.py\"\u001b[39;49;00m, line \u001b[34m862\u001b[39;49;00m, in _fp_read\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m\u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m._fp.read(amt) \u001b[34mif\u001b[39;49;00m amt \u001b[35mis\u001b[39;49;00m \u001b[35mnot\u001b[39;49;00m \u001b[34mNone\u001b[39;49;00m \u001b[34melse\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m._fp.read()\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m           \u001b[39;49;00m^^^^^^^^^^^^^^^^^^\u001b[37m\u001b[39;49;00m\n",
            "  File \u001b[36m\"/usr/lib/python3.11/http/client.py\"\u001b[39;49;00m, line \u001b[34m473\u001b[39;49;00m, in read\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00ms = \u001b[36mself\u001b[39;49;00m.fp.read(amt)\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m        \u001b[39;49;00m^^^^^^^^^^^^^^^^^\u001b[37m\u001b[39;49;00m\n",
            "  File \u001b[36m\"/usr/lib/python3.11/socket.py\"\u001b[39;49;00m, line \u001b[34m718\u001b[39;49;00m, in readinto\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m\u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m._sock.recv_into(b)\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m           \u001b[39;49;00m^^^^^^^^^^^^^^^^^^^^^^^\u001b[37m\u001b[39;49;00m\n",
            "  File \u001b[36m\"/usr/lib/python3.11/ssl.py\"\u001b[39;49;00m, line \u001b[34m1314\u001b[39;49;00m, in recv_into\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m\u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m.read(nbytes, buffer)\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m           \u001b[39;49;00m^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[37m\u001b[39;49;00m\n",
            "  File \u001b[36m\"/usr/lib/python3.11/ssl.py\"\u001b[39;49;00m, line \u001b[34m1166\u001b[39;49;00m, in read\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m    \u001b[39;49;00m\u001b[34mreturn\u001b[39;49;00m \u001b[36mself\u001b[39;49;00m._sslobj.read(\u001b[36mlen\u001b[39;49;00m, buffer)\u001b[37m\u001b[39;49;00m\n",
            "\u001b[37m           \u001b[39;49;00m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[37m\u001b[39;49;00m\n",
            "\u001b[91mKeyboardInterrupt\u001b[39;49;00m\u001b[37m\u001b[39;49;00m\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Загрузка и фильтрация данных DESI EDR"
      ],
      "metadata": {
        "id": "SpwGvxlhTI_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Источник:** DESI EDR через TAP (NOIRLab Data Lab)  \n",
        "**Таблицы:** desi_edr.zpix (z, zwarn, spectype), desi_edr.photometry (ra, dec)  \n",
        "**Фильтр:** zwarn=0, spectype≠'STAR'"
      ],
      "metadata": {
        "id": "fvh5QXxRTPWR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyvo.dal import TAPService\n",
        "\n",
        "tap = TAPService(\"https://datalab.noirlab.edu/tap\")\n",
        "query = \"\"\"\n",
        "SELECT p.ra, p.dec, z.z\n",
        "FROM desi_edr.zpix AS z\n",
        "JOIN desi_edr.photometry AS p ON z.targetid = p.targetid\n",
        "WHERE z.zwarn=0 AND z.spectype<>'STAR'\n",
        "\"\"\"\n",
        "\n",
        "logger.info(\"Отправка TAP-запроса к DESI EDR...\")\n",
        "job = tap.submit_job(query)\n",
        "job.run().wait()\n",
        "table = job.fetch_result()\n",
        "df_desi = table.to_table().to_pandas()\n",
        "desi_out = \"desi.csv\"\n",
        "df_desi.to_csv(desi_out, index=False)\n",
        "logger.info(f\"DESI EDR: получено {len(df_desi)} объектов и сохранено в {desi_out}\")\n"
      ],
      "metadata": {
        "id": "sXslXLJaTTVO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 393
        },
        "outputId": "943bdfab-2c43-440a-f64f-d3d5783f6193"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pyvo'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-a45b3aeed11d>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mpyvo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdal\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTAPService\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTAPService\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"https://datalab.noirlab.edu/tap\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m query = \"\"\"\n\u001b[1;32m      5\u001b[0m \u001b[0mSELECT\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mra\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mz\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyvo'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4. Загрузка и фильтрация данных Euclid Q1\n"
      ],
      "metadata": {
        "id": "Lrlvs9riTZbm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Источник:** Euclid Q1 через IRSA (astroquery.ipac.Irsa)  \n",
        "**Каталог:** euclid_q1_mer_catalog (содержит photoz и/или specz)  \n",
        "**Поиск:** конус вокруг центрального поля (для примера), затем можно масштабировать\n"
      ],
      "metadata": {
        "id": "KB_d7tFgThw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from astroquery.irsa import Irsa\n",
        "\n",
        "# Получим список всех доступных каталогов с «euclid» в имени\n",
        "catalogs = Irsa.list_catalogs(filter=\"euclid\")\n",
        "print(catalogs)\n"
      ],
      "metadata": {
        "id": "7Fd84SBTM_W0",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Загрузка и фильтрация данных Euclid Q1 через TAP-запрос к IRSA\n",
        "\n",
        "from astroquery.ipac.irsa import Irsa\n",
        "from astropy.coordinates import SkyCoord\n",
        "import astropy.units as u\n",
        "from astropy.table import Table\n",
        "\n",
        "# Список полей: (название, RA, Dec, радиус)\n",
        "regions = [\n",
        "    (\"EDFS\", 34.5, -4.5, 0.5),\n",
        "    (\"Euclid Deep Field North\", 34.0, 0.0, 0.5),\n",
        "    (\"CEERS\", 150.0, 2.2, 0.5),\n",
        "    (\"Fornax\", 83.8, -5.4, 0.5)\n",
        "]\n",
        "\n",
        "# Инициализируем общий результирующий каталог\n",
        "combined_table = Table(names=('ra','dec','z'), dtype=('f8','f8','f8'))\n",
        "\n",
        "total_count = 0\n",
        "for name, ra, dec, radius in regions:\n",
        "    # Создаем объект SkyCoord для центра области\n",
        "    coord = SkyCoord(ra, dec, unit='deg')\n",
        "    try:\n",
        "        # Выполняем конусный запрос к каталогу euclid_q1_mer_catalogue\n",
        "        table = Irsa.query_region(coord, catalog=\"euclid_q1_mer_catalogue\",\n",
        "                                   spatial=\"Cone\", radius=radius * u.deg)\n",
        "    except Exception as e:\n",
        "        print(f\"Ошибка запроса для области {name}: {e}\")\n",
        "        continue  # пропускаем эту область в случае ошибки\n",
        "\n",
        "    if table is None or len(table) == 0:\n",
        "        print(f\"{name}: Нет данных в указанной области (0 результатов).\")\n",
        "        continue\n",
        "    # Ищем колонку с фотометрическим z (photo-z) без учета регистра\n",
        "    z_col = None\n",
        "    for col in table.colnames:\n",
        "        if col.lower().startswith(\"photoz\") or col.lower() == \"z\":\n",
        "            z_col = col\n",
        "            break\n",
        "\n",
        "    if z_col is None:\n",
        "        print(f\"{name}: Колонка с photometric z не найдена в результатах.\")\n",
        "        continue\n",
        "\n",
        "    # Фильтруем строки с непустым значением фотометрического z\n",
        "    col_data = table[z_col]\n",
        "    if hasattr(col_data, 'mask'):  # если колонка маскирована (MaskedColumn)\n",
        "        valid_rows = ~col_data.mask  # True для тех, где не маскировано\n",
        "    else:\n",
        "        # для обычной колонки проверяем на NaN (для числовых типов)\n",
        "        valid_rows = np.isfinite(col_data)\n",
        "\n",
        "    filtered_table = table[valid_rows]\n",
        "\n",
        "    # Добавляем только столбцы ra, dec и z (photo-z) в объединенную таблицу\n",
        "    # Переименовываем колонку фотометрического z в 'z' для вывода\n",
        "    subset = filtered_table[['ra', 'dec', z_col]].copy()\n",
        "    subset.rename_column(z_col, 'z')\n",
        "    combined_table = Table.vstack([combined_table, subset])  # накапливаем результаты\n",
        "\n",
        "    # Логирование количества объектов\n",
        "    count = len(subset)\n",
        "    total_count += count\n",
        "    print(f\"{name}: найдено {count} объектов с photo-z (накопительно: {total_count}).\")\n",
        "\n",
        "# Сохраняем объединенные результаты в CSV-файл\n",
        "combined_table.write(\"euclid.csv\", format=\"ascii.csv\", overwrite=True)\n",
        "print(f\"Итого сохранено объектов: {total_count}\")\n"
      ],
      "metadata": {
        "id": "d1FigEMqTjzZ",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 5. Объединение промежуточных CSV и вычисление Cartesian coords"
      ],
      "metadata": {
        "id": "BIW9n9v6Tqh4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Склеиваем sdss.csv, desi.csv, euclid.csv в один all_galaxies.csv,\n",
        "затем добавляем столбцы x,y,z_cart (комовские координаты в Mpc)."
      ],
      "metadata": {
        "id": "Bcyo0wKhTssJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 5.1. Чтение и объединение\n",
        "files = [\"sdss.csv\", \"desi.csv\", \"euclid.csv\"]\n",
        "dfs = []\n",
        "for fn in files:\n",
        "    logger.info(f\"Чтение {fn}...\")\n",
        "    df = pd.read_csv(fn)\n",
        "    dfs.append(df)\n",
        "df_all = pd.concat(dfs, ignore_index=True)\n",
        "logger.info(f\"Объединено всего {len(df_all)} записей.\")\n",
        "\n",
        "# 5.2. Вычисление комовского расстояния и координат\n",
        "logger.info(\"Вычисление комовских координат...\")\n",
        "df_all[\"dist_mpc\"] = Planck15.comoving_distance(df_all[\"z\"]).to_value()\n",
        "ra_rad  = np.deg2rad(df_all[\"ra\"].values)\n",
        "dec_rad = np.deg2rad(df_all[\"dec\"].values)\n",
        "r       = df_all[\"dist_mpc\"].values\n",
        "df_all[\"x\"] = r * np.cos(dec_rad) * np.cos(ra_rad)\n",
        "df_all[\"y\"] = r * np.cos(dec_rad) * np.sin(ra_rad)\n",
        "df_all[\"z_cart\"] = r * np.sin(dec_rad)\n",
        "\n",
        "# 5.3. Сохранение итогового файла\n",
        "final_out = \"all_galaxies.csv\"\n",
        "df_all.to_csv(final_out, index=False)\n",
        "logger.info(f\"Итоговый файл сохранен: {final_out} (строк: {len(df_all)})\")"
      ],
      "metadata": {
        "id": "BfGb7PyaTweV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 6. Итоговая проверка ресурсов и пример"
      ],
      "metadata": {
        "id": "4CiJTdHvT5jB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Выведем несколько строк и информацию о памяти\n",
        "logger.info(\"Пример первых 5 строк итогового набора:\")\n",
        "print(df_all.head())\n",
        "\n",
        "# Проверим размер файла на диске\n",
        "size_gb = os.path.getsize(final_out) / 1e9\n",
        "logger.info(f\"Размер all_galaxies.csv: {size_gb:.2f} GB\")\n"
      ],
      "metadata": {
        "id": "P0O1BC14T__B",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        },
        "outputId": "a4903cd1-444a-4cf7-c2ab-834db8bb6d5b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'df_all' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-e6e4cbe9696b>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Выведем несколько строк и информацию о памяти\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Пример первых 5 строк итогового набора:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_all\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Проверим размер файла на диске\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'df_all' is not defined"
          ]
        }
      ]
    }
  ]
}