# Scientific API — Структура пайплайна, требования и функции

Версия: 1.0  
Проект: `scientific-api` — веб-платформа для сбора, очистки, анализа и визуализации научных данных (фокус на астрономию).

---

## 1. Общая логика пайплайна

Пайплайн устроен как **модульная цепочка шагов**, управляемых через API и веб-интерфейс:

1. **Сбор данных (Ingestion Layer)**
   - Источники:
     - Внешние API: SDSS, NASA Exoplanet Archive, ADS, arXiv, SerpAPI (Google Scholar и веб-поиск).
     - Файлы пользователя: CSV, JSON, FITS, XML, Parquet (drag-and-drop).
     - Подключение внешних баз данных.
   - Реализация:
     - `api/heavy_api.py` — тяжёлые научные эндпоинты, интеграция с внешними сервисами.
     - `api/data_management.py` — импорт/экспорт, работа с шаблонами источников.
     - `utils/data_preprocessor.py` — базовая подготовка и валидация входных данных.
   - Результат: данные приводятся к внутреннему формату и сохраняются в базу.

2. **Хранение данных (Storage Layer)**
   - Поддерживаемые backends:
     - MongoDB Atlas (основной),
     - PostgreSQL,
     - SQLite (локальная разработка / fallback).
   - Реализация:
     - `database/config.py` — менеджер подключений и выбор backend.
     - `scientific_api.db` — SQLite для локального режима.
   - Функции:
     - Инициализация схем,
     - Подключение с учётом SSL/секьюрности,
     - Абстракция над разными СУБД (единый интерфейс для API-слоя).

3. **Очистка и подготовка (Cleaning & Transformation Layer)**
   - Задачи:
     - Анализ проблем качества данных:
       - пропуски,
       - дубликаты,
       - выбросы (IQR),
       - несоответствие типов и диапазонов.
     - Выполнение операций очистки:
       - стратегия по пропускам (drop, mean/median/mode, interpolate),
       - удаление дубликатов,
       - нормализация и стандартизация,
       - удаление/обрезка выбросов,
       - приведение типов и форматов.
   - Реализация:
     - `utils/data_preprocessor.py` — поиск проблем, базовый анализ.
     - `utils/data_processing.py` — операции трансформации и очистки.
     - API-эндпоинты в `api/heavy_api.py`:
       - `/api/cleaning/analyze-issues`
       - `/api/cleaning/apply-cleaning`
   - Особенности:
     - Превью изменений перед применением.
     - Подсчёт health score (0–100).
     - Создание резервной копии перед изменениями.

4. **Аналитика и (будущие) ML-модули (Analysis & ML Layer)**
   - На текущей версии:
     - Есть инфраструктура для подключения аналитики, но ML-модули ещё в стадии TODO.
   - Планируемые задачи:
     - Кластеризация (например, галактик или объектов по признакам),
     - Классификация (типы объектов/событий),
     - Регрессия (оценка физических параметров),
     - Статистические отчёты и метрики.
   - Предполагаемая реализация:
     - Новый модуль, например `api/ml_analysis.py` + функции в `utils/data_processing.py`.
     - Хранение результатов в MongoDB/Postgres и отдача через API.

5. **Визуализация и интерфейс (Presentation Layer)**
   - Web UI:
     - `ui/dashboard.html` — основной дашборд.
     - `ui/dashboard.js` — логика фронтенда (fetch-запросы к API, модалки, подсказки).
   - Функции:
     - Пошаговый workflow: загрузка → анализ → очистка → (в будущем) анализ/ML → визуализация.
     - realtime-индикация активности, прогресса длительных задач.
   - Будущие расширения:
     - Интерктивные графики (Plotly, ECharts и др.) для научных визуализаций.
     - 3D-визуализация (например, для распределения объектов в пространстве).

6. **Оркестрация и деплой (Operations Layer)**
   - Скрипты:
     - `start_server.py` — запуск dev-сервера.
     - `start_dev_server.py` — удобный запуск в режиме разработки.
     - `manage_data_pipeline.sh` — будущий/текущий оркестратор пайплайна (cron, batch-потоки).
     - `run_tests.sh` — запуск тестов.
     - `update_azure_app.sh`, `update_azure_container.sh` — деплой и обновление на Azure.
     - `update_docker_image.sh`, `docker-compose.yml` — работа через Docker.
   - Файлы зависимостей:
     - `requirements.txt` — базовый набор.
     - `requirements-vercel.txt` — оптимизировано под Vercel.
     - `requirements_azure.txt` — под Azure.
     - `requirements_test.txt` — для тестового окружения.

---

## 2. Основные требования к пайплайну

### 2.1 Функциональные требования

1. **Сбор данных**
   - Поддержка нескольких источников (API/файлы/БД).
   - Поддержка заранее настроенных шаблонов для типичных задач:
     - `arxiv_papers`, `google_scholar`, `sdss_galaxies`, и т.д.
   - Возможность расширения (новые шаблоны и источники без перелопачивания ядра).

2. **Очистка и подготовка**
   - Стандартизированный интерфейс для анализа проблем данных.
   - Конфигурируемые стратегии обработки пропусков, выбросов и дубликатов.
   - Возможность детерминированного воспроизведения шагов (логирование и хранение параметров).

3. **Хранение**
   - Прозрачное переключение между MongoDB, PostgreSQL, SQLite.
   - Единый API-слой для чтения/записи независимо от backend.
   - Безопасное хранение строк подключения и ключей через `.env`/`config.env`.

4. **Аналитика и ML**
   - Единый контракт входных данных для ML-модулей (очищенные таблицы/матрицы признаков).
   - Конфигурируемые эксперименты (гиперпараметры, наборы features).
   - Логирование результатов и метрик.

5. **Интерфейс**
   - Визуальный контроль всех ключевых шагов пайплайна.
   - Чёткая навигация: «Сбор» → «Очистка» → «Аналитика» → «Визуализация».
   - Отображение статуса задач, ошибок, подсказок.

6. **Оптимизация и масштабирование**
   - Возможность вынести тяжёлые операции в background tasks.
   - Кэширование запросов к внешним API.
   - Rate limiting для защиты от блокировок и превышения квот.

### 2.2 Нефункциональные требования

- **Надёжность**: атомарность операций очистки (backup → transform → save).
- **Расширяемость**: новой функции достаточно добавиться в `utils/…` + отдельный endpoint.
- **Прозрачность**: детальный лог (в консоли/файле, плюс метаданные для пользователя).
- **Безопасность**:
  - Ключи и пароли только в `.env` / `config.env`.
  - Валидация входных данных на уровне FastAPI схем.
- **Производительность**:
  - Использование NumPy/Pandas/Astropy.
  - По возможности — стриминговая обработка, чтобы не хранить лишние данные в памяти и на диске.

---

## 3. Карта модулей и их роли

- `main.py`, `start_server.py`, `start_dev_server.py`
  - Точка входа, запуск FastAPI приложения `api.index:app`.

- `api/index.py`
  - Создание экземпляра приложения, регистрация роутеров (данные, очистка, тесты, и т.п.).
  - Подключение middleware (CORS, логирование, обработка ошибок).

- `api/heavy_api.py`
  - Эндпоинты, работающие с тяжёлыми вычислениями и внешними астрономическими API.

- `api/data_management.py`
  - Управление источниками данных и шаблонами.
  - Импорт/экспорт datasets, возможно подготовка к ML.

- `api/config.py`
  - Настройки API, чтение переменных окружения, глобальные параметры.

- `database/config.py`
  - Инициализация и конфигурация MongoDB/Postgres/SQLite.
  - Подключения с учётом SSL, пулов соединений и т.п.

- `utils/data_preprocessor.py`
  - Базовая очистка, детект проблем (пропуски, типы, дубликаты).

- `utils/data_processing.py`
  - Продвинутая очистка и трансформации, подготовка данных к анализу/ML.

- `ui/dashboard.html`, `ui/dashboard.js`
  - Пользовательский интерфейс для управления пайплайном.

- `manage_data_pipeline.sh`, `run_tests.sh`, `update_*.sh`
  - Автоматизация задач: тесты, деплой, обновление контейнеров/приложений.

---

## 4. Взаимодействие с внешними сервисами

- Внешние API (SDSS, NASA, ADS, arXiv, SerpAPI, Google APIs):
  - Конфигурация ключей — только через `.env` / `config.env`.
  - Взаимодействие через асинхронных клиентов (`httpx`, `aiohttp`) там, где это возможно.
  - Обработка ошибок и ретраи, логирование проблемных запросов.

- Базы данных:
  - MongoDB Atlas: основной production-вариант.
  - Cosmos DB (совместимость) — потенциальный вариант в Azure.
  - SQLite: удобен для локальной разработки и юнит-тестов.

---

## 5. Как этим пользоваться на уровне сценария

1. Настроить `.env` / `config.env` (подключения к БД, ключи).
2. Запустить `start_dev_server.py` или `uvicorn api.index:app --reload`.
3. Открыть `/docs` и протестировать API:
   - Импорт данных по шаблону.
   - Анализ проблем в наборе.
   - Применение очистки.
4. Проверить UI (`ui/dashboard.html`) и интеграцию с backend.
5. (В будущем) запускать ML-аналитики и визуализаций через отдельные эндпоинты и страницы UI.