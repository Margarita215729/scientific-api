{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "710c0a7d",
   "metadata": {},
   "source": [
    "# 01 — Космология: SDSS DR17 + DESI DR1 → унификация → подвыборки N=10³ и N=10⁴ → kNN‑графы\n",
    "\n",
    "**Задача ВКР:** построить сопоставимые графовые представления для двух физических доменов (космология и квантовые модели), чтобы сравнивать их **геометрию/спектры/топологию** единым пайплайном.\n",
    "\n",
    "В этом ноутбуке мы делаем космологическую часть **строго под требования квантовых выборок**:  \n",
    "- формируем подвыборки **ровно** размеров **N=1 000** и **N=10 000**;\n",
    "- строим kNN‑графы (узлы — объекты каталога, рёбра — ближайшие соседи в 3D‑координатах);\n",
    "- сохраняем результат в унифицированном формате `GraphData` (таблица узлов + таблица рёбер + метаданные), чтобы во втором ноутбуке **тот же код** работал и для квантовых семейств.\n",
    "\n",
    "**Артефакты:**  \n",
    "- нормализованные данные: `data/processed/cosmology/<preset>/...parquet`  \n",
    "- графы: `outputs/graphs/cosmology/<preset>/<source>/N_<N>/...`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "abf5d789",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Repo root: /workspaces/scientific-api\n",
      "Outputs: /workspaces/scientific-api/outputs\n",
      "Sample sizes: (1000, 10000)\n"
     ]
    }
   ],
   "source": [
    "# 0) Imports + reproducibility\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scientific_api.logging import setup_logging, get_logger\n",
    "from scientific_api.settings import DEFAULT_SEED, COSMO_SAMPLE_SIZES\n",
    "from scientific_api.storage.paths import ensure_all_dirs, get_repo_root, get_outputs_dir\n",
    "from scientific_api.pipeline.cosmology_ingest import run as run_cosmology_ingest\n",
    "from scientific_api.graphs.knn import GraphData, build_knn_edges, save_graphdata\n",
    "\n",
    "setup_logging(\"INFO\")\n",
    "logger = get_logger(\"nb01\")\n",
    "\n",
    "ensure_all_dirs()\n",
    "ROOT = get_repo_root()\n",
    "OUT = get_outputs_dir()\n",
    "\n",
    "print(\"Repo root:\", ROOT)\n",
    "print(\"Outputs:\", OUT)\n",
    "print(\"Sample sizes:\", COSMO_SAMPLE_SIZES)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bfeefd",
   "metadata": {},
   "source": [
    "## 1) Запуск ingestion (SDSS DR17 + DESI DR1) по пресетам\n",
    "\n",
    "Пайплайн делает:\n",
    "1. загрузку/выгрузку SDSS (через запросы) в CSV;\n",
    "2. загрузку DESI DR1 LSS clustering FITS;\n",
    "3. приведение колонок к единой схеме + фильтры по пресету;\n",
    "4. сохранение в Parquet;\n",
    "5. выравнивание размеров выборок (matched downsample), чтобы SDSS и DESI сравнивались честно.\n",
    "\n",
    "**Важно:** имена столбцов исходных таблиц могут отличаться; маппинг колонок для DESI сохраняется в `outputs/ingestion/<preset>/desi_column_mapping.json`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2a371289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-10 21:52:39 | INFO     | nb01 | Running ingestion for preset=low_z\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-10 21:52:39 | INFO     | httpx | HTTP Request: POST https://skyserver.sdss.org/dr17/SkyServerWS/SearchTools/SqlSearch \"HTTP/1.1 500 Internal Server Error\"\n",
      "2026-01-10 21:52:39 | WARNING  | scientific_api.data_sources.cosmology.sdss_dr17_sql | Попытка запроса не удалась: Server error '500 Internal Server Error' for url 'https://skyserver.sdss.org/dr17/SkyServerWS/SearchTools/SqlSearch'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
      "2026-01-10 21:52:41 | INFO     | httpx | HTTP Request: POST https://skyserver.sdss.org/dr17/SkyServerWS/SearchTools/SqlSearch \"HTTP/1.1 500 Internal Server Error\"\n",
      "2026-01-10 21:52:41 | WARNING  | scientific_api.data_sources.cosmology.sdss_dr17_sql | Попытка запроса не удалась: Server error '500 Internal Server Error' for url 'https://skyserver.sdss.org/dr17/SkyServerWS/SearchTools/SqlSearch'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
      "2026-01-10 21:52:45 | INFO     | httpx | HTTP Request: POST https://skyserver.sdss.org/dr17/SkyServerWS/SearchTools/SqlSearch \"HTTP/1.1 500 Internal Server Error\"\n",
      "2026-01-10 21:52:45 | WARNING  | scientific_api.data_sources.cosmology.sdss_dr17_sql | Попытка запроса не удалась: Server error '500 Internal Server Error' for url 'https://skyserver.sdss.org/dr17/SkyServerWS/SearchTools/SqlSearch'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n",
      "2026-01-10 21:52:53 | INFO     | httpx | HTTP Request: POST https://skyserver.sdss.org/dr17/SkyServerWS/SearchTools/SqlSearch \"HTTP/1.1 500 Internal Server Error\"\n",
      "2026-01-10 21:52:53 | WARNING  | scientific_api.data_sources.cosmology.sdss_dr17_sql | Попытка запроса не удалась: Server error '500 Internal Server Error' for url 'https://skyserver.sdss.org/dr17/SkyServerWS/SearchTools/SqlSearch'\n",
      "For more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Все попытки запросить SDSS DR17 исчерпаны: Server error '500 Internal Server Error' for url 'https://skyserver.sdss.org/dr17/SkyServerWS/SearchTools/SqlSearch'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m preset \u001b[38;5;129;01min\u001b[39;00m PRESETS:\n\u001b[32m      8\u001b[39m     logger.info(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRunning ingestion for preset=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpreset\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     meta = \u001b[43mrun_cosmology_ingest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m     ingest_meta[preset] = meta\n\u001b[32m     11\u001b[39m     \u001b[38;5;28mprint\u001b[39m(meta[\u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/scientific-api/scientific_api/pipeline/cosmology_ingest.py:132\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(preset_name)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(preset_name: \u001b[38;5;28mstr\u001b[39m) -> Dict:\n\u001b[32m    127\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Public callable for notebooks and scripts.\u001b[39;00m\n\u001b[32m    128\u001b[39m \n\u001b[32m    129\u001b[39m \u001b[33;03m    Returns a dict with paths and counts to be used downstream.\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m132\u001b[39m     summary, manifest = \u001b[43mrun_ingest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreset_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[32m    134\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mpreset\u001b[39m\u001b[33m\"\u001b[39m: preset_name,\n\u001b[32m    135\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33msummary\u001b[39m\u001b[33m\"\u001b[39m: summary,\n\u001b[32m    136\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmanifest\u001b[39m\u001b[33m\"\u001b[39m: manifest.model_dump() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(manifest, \u001b[33m\"\u001b[39m\u001b[33mmodel_dump\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m manifest.\u001b[34m__dict__\u001b[39m,\n\u001b[32m    137\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/scientific-api/scientific_api/pipeline/cosmology_ingest.py:58\u001b[39m, in \u001b[36mrun_ingest\u001b[39m\u001b[34m(preset_name)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m# SDSS\u001b[39;00m\n\u001b[32m     57\u001b[39m sdss_raw_dir = ROOT / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mraw\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33msdss\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mdr17\u001b[39m\u001b[33m\"\u001b[39m / preset_name\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m sdss_raw_path = \u001b[43mfetch_sdss_dr17_points_rect_tiled\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreset_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msdss_raw_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     59\u001b[39m n_sdss_raw = \u001b[38;5;28mlen\u001b[39m(pd.read_csv(sdss_raw_path))\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# DESI\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/scientific-api/scientific_api/data_sources/cosmology/sdss_dr17_sql.py:109\u001b[39m, in \u001b[36mfetch_sdss_dr17_points_rect_tiled\u001b[39m\u001b[34m(preset, preset_name, out_raw_dir)\u001b[39m\n\u001b[32m    107\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    108\u001b[39m sql = _build_sql(tile, preset)\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m df = \u001b[43m_run_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m df.empty \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(df.columns) == \u001b[32m0\u001b[39m:\n\u001b[32m    111\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSDSS ответ без колонок для тайла \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/scientific-api/scientific_api/data_sources/cosmology/sdss_dr17_sql.py:55\u001b[39m, in \u001b[36m_run_query\u001b[39m\u001b[34m(sql)\u001b[39m\n\u001b[32m     53\u001b[39m         last_exc = exc\n\u001b[32m     54\u001b[39m         logger.warning(\u001b[33m\"\u001b[39m\u001b[33mПопытка запроса не удалась: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, exc)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mВсе попытки запросить SDSS DR17 исчерпаны: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlast_exc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mRuntimeError\u001b[39m: Все попытки запросить SDSS DR17 исчерпаны: Server error '500 Internal Server Error' for url 'https://skyserver.sdss.org/dr17/SkyServerWS/SearchTools/SqlSearch'\nFor more information check: https://developer.mozilla.org/en-US/docs/Web/HTTP/Status/500"
     ]
    }
   ],
   "source": [
    "# 1) Ingest both presets used in Chapter 1 logic\n",
    "# We run both low_z and high_z to obtain more diverse cosmology corpora.\n",
    "\n",
    "PRESETS = [\"low_z\", \"high_z\"]\n",
    "ingest_meta = {}\n",
    "\n",
    "for preset in PRESETS:\n",
    "    logger.info(f\"Running ingestion for preset={preset}\")\n",
    "    meta = run_cosmology_ingest(preset)\n",
    "    ingest_meta[preset] = meta\n",
    "    print(meta[\"summary\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f17621",
   "metadata": {},
   "source": [
    "## 2) Загрузка нормализованных matched‑выборок и проверка схемы\n",
    "\n",
    "Единая схема (см. `scientific_api.data_processing.cosmology.schema`):\n",
    "`source, sample, obj_id, ra_deg, dec_deg, z, x_mpc, y_mpc, z_mpc, weight`.\n",
    "\n",
    "Мы **не предполагаем**, что сырой SDSS/DESI имеют эти имена — мы проверяем **после нормализации**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b68fb1c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Missing matched parquet: /workspaces/scientific-api/data/processed/cosmology/low_z/sdss_dr17__matched.parquet",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     13\u001b[39m cosmo = {}\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m preset \u001b[38;5;129;01min\u001b[39;00m PRESETS:\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m     cosmo[(preset, \u001b[33m\"\u001b[39m\u001b[33msdss_dr17\u001b[39m\u001b[33m\"\u001b[39m)] = \u001b[43mload_matched_points\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msdss_dr17\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     16\u001b[39m     cosmo[(preset, \u001b[33m\"\u001b[39m\u001b[33mdesi_dr1\u001b[39m\u001b[33m\"\u001b[39m)] = load_matched_points(preset, \u001b[33m\"\u001b[39m\u001b[33mdesi_dr1\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (preset, source), df \u001b[38;5;129;01min\u001b[39;00m cosmo.items():\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36mload_matched_points\u001b[39m\u001b[34m(preset, source)\u001b[39m\n\u001b[32m      4\u001b[39m path = ROOT / \u001b[33m\"\u001b[39m\u001b[33mdata\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mprocessed\u001b[39m\u001b[33m\"\u001b[39m / \u001b[33m\"\u001b[39m\u001b[33mcosmology\u001b[39m\u001b[33m\"\u001b[39m / preset / \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msource\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m__matched.parquet\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m path.exists():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMissing matched parquet: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m df = pd.read_parquet(path).reset_index(drop=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m      8\u001b[39m missing = [c \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m REQUIRED_COLUMNS \u001b[38;5;28;01mif\u001b[39;00m c \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m df.columns]\n",
      "\u001b[31mFileNotFoundError\u001b[39m: Missing matched parquet: /workspaces/scientific-api/data/processed/cosmology/low_z/sdss_dr17__matched.parquet"
     ]
    }
   ],
   "source": [
    "from scientific_api.data_processing.cosmology.schema import REQUIRED_COLUMNS\n",
    "\n",
    "def load_matched_points(preset: str, source: str) -> pd.DataFrame:\n",
    "    path = ROOT / \"data\" / \"processed\" / \"cosmology\" / preset / f\"{source}__matched.parquet\"\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing matched parquet: {path}\")\n",
    "    df = pd.read_parquet(path).reset_index(drop=True)\n",
    "    missing = [c for c in REQUIRED_COLUMNS if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{path} missing required columns: {missing}\")\n",
    "    return df\n",
    "\n",
    "cosmo = {}\n",
    "for preset in PRESETS:\n",
    "    cosmo[(preset, \"sdss_dr17\")] = load_matched_points(preset, \"sdss_dr17\")\n",
    "    cosmo[(preset, \"desi_dr1\")] = load_matched_points(preset, \"desi_dr1\")\n",
    "\n",
    "for (preset, source), df in cosmo.items():\n",
    "    print(f\"{preset:6s} {source:9s}  n={len(df):7d}  z-range=({df['z'].min():.3f},{df['z'].max():.3f})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93155ba5",
   "metadata": {},
   "source": [
    "## 3) Подвыборки фиксированного размера N=1 000 и N=10 000\n",
    "\n",
    "Ключевое требование для корректного сравнения: **размер графа** должен быть сопоставим между доменами.  \n",
    "Поэтому здесь мы строим 4 графа на каждый пресет:\n",
    "- SDSS: N=1k, N=10k  \n",
    "- DESI: N=1k, N=10k\n",
    "\n",
    "Подвыборки делаем **детерминированно**: фиксируем seed, сохраняем индексы в `meta`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a544edef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixed_sample(df: pd.DataFrame, n: int, seed: int) -> pd.DataFrame:\n",
    "    if len(df) < n:\n",
    "        raise ValueError(f\"Requested n={n}, but dataset has only {len(df)} rows.\")\n",
    "    rng = np.random.default_rng(seed)\n",
    "    idx = rng.choice(len(df), size=n, replace=False)\n",
    "    sub = df.iloc[idx].copy().reset_index(drop=True)\n",
    "    sub[\"node_id\"] = np.arange(n, dtype=int)\n",
    "    sub.attrs[\"sample_indices\"] = idx.tolist()\n",
    "    return sub\n",
    "\n",
    "samples = {}  # (preset, source, N) -> df\n",
    "for preset in PRESETS:\n",
    "    for source in [\"sdss_dr17\", \"desi_dr1\"]:\n",
    "        df = cosmo[(preset, source)]\n",
    "        for N in COSMO_SAMPLE_SIZES:\n",
    "            sub = fixed_sample(df, N, seed=DEFAULT_SEED + hash((preset, source, N)) % 10_000)\n",
    "            samples[(preset, source, N)] = sub\n",
    "            print(f\"sampled {preset} {source} N={N} -> {len(sub)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af36e2ab",
   "metadata": {},
   "source": [
    "## 4) Построение kNN‑графов и сохранение в `GraphData`\n",
    "\n",
    "- Узлы: строки DataFrame (координаты `x_mpc,y_mpc,z_mpc`, физические поля, и т.д.)\n",
    "- Рёбра: `k` ближайших соседей по евклидовой метрике в 3D\n",
    "- Вес ребра: расстояние (потом можно нормировать/инвертировать при необходимости)\n",
    "\n",
    "Сохраняем:\n",
    "`outputs/graphs/cosmology/<preset>/<source>/N_<N>/{nodes.parquet, edges.parquet, meta.json}`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083b115f",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 12  # fixed for the project (no knobs in the notebook)\n",
    "\n",
    "graph_index_rows = []\n",
    "\n",
    "for (preset, source, N), df in samples.items():\n",
    "    coords = df[[\"x_mpc\", \"y_mpc\", \"z_mpc\"]].to_numpy(dtype=float)\n",
    "    edges = build_knn_edges(coords, k=K, metric=\"euclidean\")\n",
    "\n",
    "    nodes = df.copy()\n",
    "    # ensure required node_id\n",
    "    if \"node_id\" not in nodes.columns:\n",
    "        nodes[\"node_id\"] = np.arange(len(nodes), dtype=int)\n",
    "\n",
    "    meta = {\n",
    "        \"domain\": \"cosmology\",\n",
    "        \"preset\": preset,\n",
    "        \"source\": source,\n",
    "        \"N\": int(N),\n",
    "        \"k\": int(K),\n",
    "        \"seed\": int(DEFAULT_SEED),\n",
    "        \"coord_cols\": [\"x_mpc\", \"y_mpc\", \"z_mpc\"],\n",
    "        \"sample_indices\": df.attrs.get(\"sample_indices\", None),\n",
    "    }\n",
    "\n",
    "    g = GraphData(nodes=nodes, edges=edges, meta=meta)\n",
    "\n",
    "    out_dir = OUT / \"graphs\" / \"cosmology\" / preset / source / f\"N_{N}\"\n",
    "    save_graphdata(g, out_dir)\n",
    "\n",
    "    graph_index_rows.append({\n",
    "        \"domain\": \"cosmology\",\n",
    "        \"preset\": preset,\n",
    "        \"source\": source,\n",
    "        \"N\": N,\n",
    "        \"k\": K,\n",
    "        \"path\": str(out_dir),\n",
    "        \"n_edges\": int(edges.shape[0]),\n",
    "    })\n",
    "\n",
    "graph_index = pd.DataFrame(graph_index_rows).sort_values([\"preset\",\"source\",\"N\"])\n",
    "graph_index_path = OUT / \"graphs\" / \"graph_index_cosmology.parquet\"\n",
    "graph_index.to_parquet(graph_index_path, index=False)\n",
    "\n",
    "graph_index\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1cba9a",
   "metadata": {},
   "source": [
    "## 5) Визуальная проверка: 3D‑облако и распределение степеней (N=1000)\n",
    "\n",
    "Мы делаем быструю sanity‑проверку:\n",
    "- точки не вырожденные;\n",
    "- kNN‑граф не пустой;\n",
    "- степень узлов имеет ожидаемый порядок (в среднем около k).\n",
    "\n",
    "Показываем по одному примеру SDSS и DESI для каждого пресета.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e81d8c18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scientific_api.graphs.knn import load_graphdata\n",
    "import networkx as nx\n",
    "\n",
    "def edges_to_nx(n: int, edges: pd.DataFrame) -> nx.Graph:\n",
    "    G = nx.Graph()\n",
    "    G.add_nodes_from(range(n))\n",
    "    if len(edges) > 0:\n",
    "        G.add_weighted_edges_from(edges[[\"source\",\"target\",\"weight\"]].itertuples(index=False, name=None))\n",
    "    return G\n",
    "\n",
    "for preset in PRESETS:\n",
    "    fig = plt.figure(figsize=(12, 5))\n",
    "    for j, source in enumerate([\"sdss_dr17\", \"desi_dr1\"]):\n",
    "        gdir = OUT / \"graphs\" / \"cosmology\" / preset / source / \"N_1000\"\n",
    "        gd = load_graphdata(gdir)\n",
    "        nodes = gd.nodes\n",
    "        edges = gd.edges\n",
    "\n",
    "        ax = fig.add_subplot(1, 2, j+1, projection=\"3d\")\n",
    "        ax.scatter(nodes[\"x_mpc\"], nodes[\"y_mpc\"], nodes[\"z_mpc\"], s=2)\n",
    "        ax.set_title(f\"{preset} | {source} | N=1000\")\n",
    "        ax.set_xlabel(\"x_mpc\"); ax.set_ylabel(\"y_mpc\"); ax.set_zlabel(\"z_mpc\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Degree hist\n",
    "    fig = plt.figure(figsize=(12, 4))\n",
    "    for j, source in enumerate([\"sdss_dr17\", \"desi_dr1\"]):\n",
    "        gdir = OUT / \"graphs\" / \"cosmology\" / preset / source / \"N_1000\"\n",
    "        gd = load_graphdata(gdir)\n",
    "        G = edges_to_nx(len(gd.nodes), gd.edges)\n",
    "        deg = np.array([d for _, d in G.degree()], dtype=int)\n",
    "\n",
    "        ax = fig.add_subplot(1, 2, j+1)\n",
    "        ax.hist(deg, bins=30)\n",
    "        ax.set_title(f\"Degree hist: {preset} | {source} | mean={deg.mean():.2f}\")\n",
    "        ax.set_xlabel(\"degree\"); ax.set_ylabel(\"count\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e847f8",
   "metadata": {},
   "source": [
    "## 6) Что дальше\n",
    "\n",
    "- В этом ноутбуке получены космологические графы **строго** размеров `N=1000` и `N=10000`.\n",
    "- Во втором ноутбуке мы:\n",
    "  1) генерируем **квантовые ансамбли** на тех же N и с контролируемыми параметрами (W, p);  \n",
    "  2) извлекаем единый набор признаков;  \n",
    "  3) сравниваем домены (космология vs квантовые семейства) по структурным и спектральным метрикам.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
